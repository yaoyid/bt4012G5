{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessed_data_with_num.csv')\n",
    "data = data.drop(columns= ['missing company profile',\n",
    "       'missing location', 'missing department',  'missing requirements','missing benefits', 'full_text_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all text\n",
    "data['full_text'] = data['title'] + \" \" + data['location']  + \" \" + data['department']  + \" \" + data['company_profile']  + \" \" + data['description']  + \" \" + data['requirements']  + \" \"  + data['benefits'] + data['industry']  + \" \" + data['function']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Models with categorical features</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier ,ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier ,ExtraTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score ,recall_score,precision_score,f1_score ,classification_report, r2_score ,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(['fraudulent', 'in_balanced_dataset' ] , axis = 1)\n",
    "y = data['fraudulent']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3 , random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = x_train.copy()\n",
    "train_set['fraudulent'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_selected = x_train.loc[:, [ 'has_questions', 'employment_type',\n",
    "       'required_experience', 'required_education', 'missing_combination_Both profile and logo Missing', 'missing_combination_Both profile and logo Present',\n",
    "       'missing_combination_Missing Logo Only', 'missing_combination_Missing Profile Only',\n",
    "       'title_length', 'department_length',\n",
    "       'company_profile_length', 'description_length', 'requirements_length']]\n",
    "\n",
    "x_test_selected = x_test.loc[:, [ 'has_questions', 'employment_type',\n",
    "       'required_experience', 'required_education', 'missing_combination_Both profile and logo Missing', 'missing_combination_Both profile and logo Present',\n",
    "       'missing_combination_Missing Logo Only', 'missing_combination_Missing Profile Only',\n",
    "       'title_length', 'department_length',\n",
    "       'company_profile_length', 'description_length', 'requirements_length']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression(max_iter=2500) _ Train Details</th>\n",
       "      <td>0.874220</td>\n",
       "      <td>0.336608</td>\n",
       "      <td>0.225014</td>\n",
       "      <td>0.667797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression(max_iter=2500) _ Test Details</th>\n",
       "      <td>0.869426</td>\n",
       "      <td>0.329777</td>\n",
       "      <td>0.222805</td>\n",
       "      <td>0.634328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Accuracy  F1_score  \\\n",
       "LogisticRegression(max_iter=2500) _ Train Details  0.874220  0.336608   \n",
       "LogisticRegression(max_iter=2500) _ Test Details   0.869426  0.329777   \n",
       "\n",
       "                                                   Precision    Recall  \n",
       "LogisticRegression(max_iter=2500) _ Train Details   0.225014  0.667797  \n",
       "LogisticRegression(max_iter=2500) _ Test Details    0.222805  0.634328  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "classification_report = pd.DataFrame(columns=['Accuracy','F1_score','Precision','Recall'])\n",
    "\n",
    "model = LogisticRegression(max_iter= 2500)\n",
    "#max_iter is set as the model fails to converge when the number is too low. \n",
    "model = model.fit(x_train_selected , y_train)\n",
    "for i in range(2) :\n",
    "    if i == 0 :\n",
    "        to_pred = x_train_selected\n",
    "        pred = y_train\n",
    "        title = 'Train'\n",
    "        \n",
    "    else :\n",
    "        to_pred = x_test_selected\n",
    "        pred = y_test\n",
    "        title = 'Test'\n",
    "    # Assuming 'model' is your trained logistic regression model\n",
    "    threshold = 0.10  # Adjust the threshold as needed\n",
    "    y_pred = (model.predict_proba(to_pred)[:, 1] > threshold).astype(int)\n",
    "\n",
    "    # y_pred = model.predict(to_pred)\n",
    "\n",
    "    acc = accuracy_score(pred , y_pred)\n",
    "    f1 = f1_score(pred , y_pred)\n",
    "    prec = precision_score(pred , y_pred)\n",
    "    recall = recall_score(pred , y_pred)\n",
    "    d = pd.DataFrame(data=np.array([acc,f1,prec,recall]).reshape(1,4) \n",
    "                    , columns=['Accuracy' , 'F1_score' , 'Precision' , 'Recall'])  \n",
    "    classification_report = pd.concat([classification_report , d])\n",
    "    classification_report.rename( index= { 0 :'{} _ {} Details'.format(model , title) } ,inplace=True )\n",
    "\n",
    "classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC(probability=True) _ Train Details</th>\n",
       "      <td>0.952539</td>\n",
       "      <td>0.013468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC(probability=True) _ Test Details</th>\n",
       "      <td>0.948791</td>\n",
       "      <td>0.007326</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.003731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Accuracy  F1_score  Precision    Recall\n",
       "SVC(probability=True) _ Train Details  0.952539  0.013468        1.0  0.006780\n",
       "SVC(probability=True) _ Test Details   0.948791  0.007326        0.2  0.003731"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "classification_report = pd.DataFrame(columns=['Accuracy','F1_score','Precision','Recall'])\n",
    "\n",
    "model = SVC(probability= True)\n",
    "model = model.fit(x_train_selected , y_train)\n",
    "for i in range(2) :\n",
    "    if i == 0 :\n",
    "        to_pred = x_train_selected\n",
    "        pred = y_train\n",
    "        title = 'Train'\n",
    "        \n",
    "    else :\n",
    "        to_pred = x_test_selected\n",
    "        pred = y_test\n",
    "        title = 'Test'\n",
    "    threshold = 0.10  # Adjust the threshold as needed\n",
    "    y_pred = (model.predict_proba(to_pred)[:, 1] > threshold).astype(int)\n",
    "\n",
    "    acc = accuracy_score(pred , y_pred)\n",
    "    f1 = f1_score(pred , y_pred)\n",
    "    prec = precision_score(pred , y_pred)\n",
    "    recall = recall_score(pred , y_pred)\n",
    "    d = pd.DataFrame(data=np.array([acc,f1,prec,recall]).reshape(1,4) \n",
    "                    , columns=['Accuracy' , 'F1_score' , 'Precision' , 'Recall'])  \n",
    "    classification_report = pd.concat([classification_report , d])\n",
    "    classification_report.rename( index= { 0 :'{} _ {} Details'.format(model , title) } ,inplace=True )\n",
    "\n",
    "classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model has a relatively poor performance, possibly due to that fact that the data is not linearly separable enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier() _ Train Details</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier() _ Test Details</th>\n",
       "      <td>0.957861</td>\n",
       "      <td>0.595281</td>\n",
       "      <td>0.579505</td>\n",
       "      <td>0.61194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Accuracy  F1_score  Precision  \\\n",
       "DecisionTreeClassifier() _ Train Details  1.000000  1.000000   1.000000   \n",
       "DecisionTreeClassifier() _ Test Details   0.957861  0.595281   0.579505   \n",
       "\n",
       "                                           Recall  \n",
       "DecisionTreeClassifier() _ Train Details  1.00000  \n",
       "DecisionTreeClassifier() _ Test Details   0.61194  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report = pd.DataFrame(columns=['Accuracy','F1_score','Precision','Recall'])\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model = model.fit(x_train_selected , y_train)\n",
    "for i in range(2) :\n",
    "    if i == 0 :\n",
    "        to_pred = x_train_selected\n",
    "        pred = y_train\n",
    "        title = 'Train'\n",
    "        \n",
    "    else :\n",
    "        to_pred = x_test_selected\n",
    "        pred = y_test\n",
    "        title = 'Test'\n",
    "    threshold = 0.10  # Adjust the threshold as needed\n",
    "    y_pred = (model.predict_proba(to_pred)[:, 1] > threshold).astype(int)\n",
    "\n",
    "\n",
    "    acc = accuracy_score(pred , y_pred)\n",
    "    f1 = f1_score(pred , y_pred)\n",
    "    prec = precision_score(pred , y_pred)\n",
    "    recall = recall_score(pred , y_pred)\n",
    "    d = pd.DataFrame(data=np.array([acc,f1,prec,recall]).reshape(1,4) \n",
    "                    , columns=['Accuracy' , 'F1_score' , 'Precision' , 'Recall'])  \n",
    "    classification_report = pd.concat([classification_report , d])\n",
    "    classification_report.rename( index= { 0 :'{} _ {} Details'.format(model , title) } ,inplace=True )\n",
    "\n",
    "classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier() _ Train Details</th>\n",
       "      <td>0.981858</td>\n",
       "      <td>0.840456</td>\n",
       "      <td>0.724816</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier() _ Test Details</th>\n",
       "      <td>0.928005</td>\n",
       "      <td>0.554386</td>\n",
       "      <td>0.403748</td>\n",
       "      <td>0.884328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Accuracy  F1_score  Precision  \\\n",
       "RandomForestClassifier() _ Train Details  0.981858  0.840456   0.724816   \n",
       "RandomForestClassifier() _ Test Details   0.928005  0.554386   0.403748   \n",
       "\n",
       "                                            Recall  \n",
       "RandomForestClassifier() _ Train Details  1.000000  \n",
       "RandomForestClassifier() _ Test Details   0.884328  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "classification_report = pd.DataFrame(columns=['Accuracy','F1_score','Precision','Recall'])\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model = model.fit(x_train_selected , y_train)\n",
    "for i in range(2) :\n",
    "    if i == 0 :\n",
    "        to_pred = x_train_selected\n",
    "        pred = y_train\n",
    "        title = 'Train'\n",
    "        \n",
    "    else :\n",
    "        to_pred = x_test_selected\n",
    "        pred = y_test\n",
    "        title = 'Test'\n",
    "    \n",
    "    threshold = 0.10  # Adjust the threshold as needed\n",
    "    y_pred = (model.predict_proba(to_pred)[:, 1] > threshold).astype(int)\n",
    "\n",
    "    acc = accuracy_score(pred , y_pred)\n",
    "    f1 = f1_score(pred , y_pred)\n",
    "    prec = precision_score(pred , y_pred)\n",
    "    recall = recall_score(pred , y_pred)\n",
    "    d = pd.DataFrame(data=np.array([acc,f1,prec,recall]).reshape(1,4) \n",
    "                    , columns=['Accuracy' , 'F1_score' , 'Precision' , 'Recall'])  \n",
    "    classification_report = pd.concat([classification_report , d])\n",
    "    classification_report.rename( index= { 0 :'{} _ {} Details'.format(model , title) } ,inplace=True )\n",
    "\n",
    "classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBClassifier(base_score=None, booster=None, callbacks=None,\\n              colsample_bylevel=None, colsample_bynode=None,\\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\\n              enable_categorical=False, eval_metric=None, feature_types=None,\\n              gamma=None, grow_policy=None, importance_type=None,\\n              interaction_constraints=None, learning_rate=None, max_bin=None,\\n              max_cat_threshold=None, max_cat_to_onehot=None,\\n              max_delta_step=None, max_depth=None, max_leaves=None,\\n              min_child_weight=None, missing=nan, monotone_constraints=None,\\n              multi_strategy=None, n_estimators=None, n_jobs=None,\\n              num_parallel_tree=None, random_state=None, ...) _ Train Details</th>\n",
       "      <td>0.976998</td>\n",
       "      <td>0.806011</td>\n",
       "      <td>0.675057</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier(base_score=None, booster=None, callbacks=None,\\n              colsample_bylevel=None, colsample_bynode=None,\\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\\n              enable_categorical=False, eval_metric=None, feature_types=None,\\n              gamma=None, grow_policy=None, importance_type=None,\\n              interaction_constraints=None, learning_rate=None, max_bin=None,\\n              max_cat_threshold=None, max_cat_to_onehot=None,\\n              max_delta_step=None, max_depth=None, max_leaves=None,\\n              min_child_weight=None, missing=nan, monotone_constraints=None,\\n              multi_strategy=None, n_estimators=None, n_jobs=None,\\n              num_parallel_tree=None, random_state=None, ...) _ Test Details</th>\n",
       "      <td>0.949169</td>\n",
       "      <td>0.609579</td>\n",
       "      <td>0.498812</td>\n",
       "      <td>0.783582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Accuracy  F1_score  \\\n",
       "XGBClassifier(base_score=None, booster=None, ca...  0.976998  0.806011   \n",
       "XGBClassifier(base_score=None, booster=None, ca...  0.949169  0.609579   \n",
       "\n",
       "                                                    Precision    Recall  \n",
       "XGBClassifier(base_score=None, booster=None, ca...   0.675057  1.000000  \n",
       "XGBClassifier(base_score=None, booster=None, ca...   0.498812  0.783582  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "classification_report = pd.DataFrame(columns=['Accuracy','F1_score','Precision','Recall'])\n",
    "\n",
    "model = XGBClassifier()\n",
    "model = model.fit(x_train_selected , y_train)\n",
    "for i in range(2) :\n",
    "    if i == 0 :\n",
    "        to_pred = x_train_selected\n",
    "        pred = y_train\n",
    "        title = 'Train'\n",
    "        \n",
    "    else :\n",
    "        to_pred = x_test_selected\n",
    "        pred = y_test\n",
    "        title = 'Test'\n",
    "    \n",
    "    threshold = 0.10  # Adjust the threshold as needed\n",
    "    y_pred = (model.predict_proba(to_pred)[:, 1] > threshold).astype(int)\n",
    "\n",
    "    acc = accuracy_score(pred , y_pred)\n",
    "    f1 = f1_score(pred , y_pred)\n",
    "    prec = precision_score(pred , y_pred)\n",
    "    recall = recall_score(pred , y_pred)\n",
    "    d = pd.DataFrame(data=np.array([acc,f1,prec,recall]).reshape(1,4) \n",
    "                    , columns=['Accuracy' , 'F1_score' , 'Precision' , 'Recall'])  \n",
    "    classification_report = pd.concat([classification_report , d])\n",
    "    classification_report.rename( index= { 0 :'{} _ {} Details'.format(model , title) } ,inplace=True )\n",
    "\n",
    "classification_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Logistic Regression | SVM | Decision Tree | Random Forest | XGBoost | Average |\n",
    "|----------|----------|----------|----------|----------|----------|----------|\n",
    "| test result | 0.330 | 0.00733 | 0.595 | 0.554 | 0.610 | 0.420 |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Only Models</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test_full,y_train , y_test = train_test_split(data.drop('fraudulent', axis=1), data[\"fraudulent\"], test_size=0.3, random_state=0)\n",
    "X_train = X_train_full[['full_text', 'has_questions', 'has_company_logo', 'employment_type', 'required_experience', 'required_education']]\n",
    "X_test = X_test_full[['full_text', 'has_questions', 'has_company_logo', 'employment_type', 'required_experience', 'required_education']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['full_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.552278820375335\n",
      "SVM F1 Score: 0.7129186602870813\n",
      "Decision Tree F1 Score: 0.7455621301775147\n",
      "Random Forest F1 Score: 0.7505827505827506\n",
      "XGBoost F1 Score: 0.7849223946784921\n",
      "average Score: 0.7092529512202347\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "logreg_model = LogisticRegression(max_iter= 1500)\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_f1 = train_and_evaluate_model(dt_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"Decision Tree F1 Score: {dt_f1}\")\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")\n",
    "\n",
    "print(f\"average Score: {np.mean([logreg_f1, svm_f1, dt_f1, rf_f1, xgb_f1])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenized_text = X_train['full_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert words to vectors\n",
    "def get_vector(word_list, model):\n",
    "    valid_words = [word for word in word_list if word in model.wv]\n",
    "    if not valid_words:\n",
    "        # If no valid words, return a vector of zeros or handle as needed\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model.wv[word] for word in valid_words], axis=0)\n",
    "\n",
    "X_train_word2vec = tokenized_text.apply(lambda x: get_vector(x, word2vec_model))\n",
    "X_test_word2vec = X_test['full_text'].apply(lambda x: get_vector(x.split(), word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.3268698060941828\n",
      "SVM F1 Score: 0.4550561797752809\n",
      "Decision Tree F1 Score: 0.43868739205526774\n",
      "Random Forest F1 Score: 0.532258064516129\n",
      "XGBoost F1 Score: 0.6339066339066338\n",
      "average Score: 0.4773556152694988\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train.to_list(), y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test.to_list())\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "logreg_model = LogisticRegression(max_iter= 1500)\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_f1 = train_and_evaluate_model(dt_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"Decision Tree F1 Score: {dt_f1}\")\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")\n",
    "\n",
    "print(f\"average Score: {np.mean([logreg_f1, svm_f1, dt_f1, rf_f1, xgb_f1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngrams Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train into fraud and non-fraud\n",
    "X_train_nonfraud = X_train.loc[y_train==0]\n",
    "X_train_fraud = X_train.loc[y_train==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to return ngrams sorted by frequency\n",
    "def get_ngrams(ngram, corpus):\n",
    "    vec = CountVectorizer(ngram_range=(ngram, ngram)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = {}\n",
    "    for word, idx in vec.vocabulary_.items():\n",
    "        words_freq[word] = sum_words[0, idx]\n",
    "    words_freq = dict(sorted(words_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('awesome', 1362), ('abroad', 1289), ('athens', 921), ('european', 865), ('berlin', 785)]\n",
      "[('aker', 152), ('accion', 58), ('0fa3f7c5e23a16de16a841e368006cae916884407d90b154dfef3976483a71ae', 53), ('anyperk', 40), ('novation', 40)]\n",
      "[('engineering', 0.16199194664859687, 0.4842268371145133, 0.32223489046591647), ('team', 0.8978269988982973, 0.5835986054267092, 0.31422839347158815), ('position', 0.26084672032725065, 0.5297020531217894, 0.26885533279453877), ('marketing', 0.32690296259294693, 0.10189816920148889, 0.22500479339145804), ('work', 0.884937176053005, 1.0998265204722686, 0.21488934441926355)]\n"
     ]
    }
   ],
   "source": [
    "nonfraud_unigram = get_ngrams(1, X_train_nonfraud['full_text'])\n",
    "fraud_unigram = get_ngrams(1, X_train_fraud['full_text'])\n",
    "nonfraud_unigram_top5 = [(k,v) for k,v in nonfraud_unigram.items() if k not in fraud_unigram.keys()][:5]\n",
    "fraud_unigram_top5 = [(k,v) for k,v in fraud_unigram.items() if k not in nonfraud_unigram.keys()][:5]\n",
    "\n",
    "nonfraud_num_unigram, fraud_num_unigram = sum(nonfraud_unigram.values()), sum(fraud_unigram.values())\n",
    "diff_unigram = [(k, nonfraud_unigram[k]*100/nonfraud_num_unigram, fraud_unigram[k]*100/fraud_num_unigram, \n",
    "                 abs((nonfraud_unigram[k]*100/nonfraud_num_unigram)-(fraud_unigram[k]*100/fraud_num_unigram))) for k in nonfraud_unigram.keys() if k in fraud_unigram.keys()]\n",
    "diff_unigram = sorted(diff_unigram, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "print(nonfraud_unigram_top5)\n",
    "print(fraud_unigram_top5)\n",
    "print(diff_unigram[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('university degree', 820), ('increase productivity', 783), ('document communication', 773), ('relevant job', 709), ('digital marketing', 617)]\n",
      "[('aker solution', 148), ('aptitude staffing', 76), ('bring discovery', 53), ('production maximize', 53), ('maximize recovery', 53)]\n",
      "[('data entry', 0.008330150535368569, 0.1887335387115339, 0.18040338817616533), ('oil gas', 0.011754402138840232, 0.12610447205389486, 0.11435006991505463), ('customer service', 0.15939232704236853, 0.2657503639256576, 0.10635803688328907), ('work home', 0.006914354199317784, 0.09648261620230882, 0.08956826200299103), ('gas industry', 0.005498557863267, 0.08717289007752463, 0.08167433221425763)]\n"
     ]
    }
   ],
   "source": [
    "nonfraud_bigram = get_ngrams(2, X_train_nonfraud['full_text'])\n",
    "fraud_bigram = get_ngrams(2,  X_train_fraud['full_text'])\n",
    "nonfraud_bigram_top5 = [(k,v) for k,v in nonfraud_bigram.items() if k not in fraud_bigram.keys()][:5]\n",
    "fraud_bigram_top5 = [(k,v) for k,v in fraud_bigram.items() if k not in nonfraud_bigram.keys()][:5]\n",
    "\n",
    "nonfraud_num_bigram, fraud_num_bigram = sum(nonfraud_bigram.values()), sum(fraud_bigram.values())\n",
    "diff_bigram = [(k, nonfraud_bigram[k]*100/nonfraud_num_bigram, fraud_bigram[k]*100/fraud_num_bigram, \n",
    "                abs((nonfraud_bigram[k]*100/nonfraud_num_bigram)-(fraud_bigram[k]*100/fraud_num_bigram))) for k in nonfraud_bigram.keys() if k in fraud_bigram.keys()]\n",
    "diff_bigram = sorted(diff_bigram, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "print(nonfraud_bigram_top5)\n",
    "print(fraud_bigram_top5)\n",
    "print(diff_bigram[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('full time permanent', 587), ('time permanent position', 563), ('permanent position many', 550), ('position many medium', 550), ('many medium large', 550)]\n",
      "[('gas industry engineering', 55), ('28 000 people', 55), ('aker solution global', 53), ('solution global provider', 53), ('global provider product', 53)]\n",
      "[('oil gas industry', 0.00551992577517772, 0.0876103635404794, 0.08209043776530167), ('usa tx houston', 0.0038341999396444043, 0.05103516322746372, 0.047200963287819316), ('product system service', 3.305344775555521e-05, 0.04508106085092629, 0.045048007403170734), ('approximately 28 000', 6.610689551111042e-05, 0.04508106085092629, 0.045014953955415174), ('service oil gas', 0.0003635879253111073, 0.04508106085092629, 0.04471747292561518)]\n"
     ]
    }
   ],
   "source": [
    "nonfraud_trigram = get_ngrams(3,  X_train_nonfraud['full_text'])\n",
    "fraud_trigram = get_ngrams(3,  X_train_fraud['full_text'])\n",
    "nonfraud_trigram_top5 = [(k,v) for k,v in nonfraud_trigram.items() if k not in fraud_trigram.keys()][:5]\n",
    "fraud_trigram_top5 = [(k,v) for k,v in fraud_trigram.items() if k not in nonfraud_trigram.keys()][:5]\n",
    "\n",
    "nonfraud_num_trigram, fraud_num_trigram = sum(nonfraud_trigram.values()), sum(fraud_trigram.values())\n",
    "diff_trigram = [(k, nonfraud_trigram[k]*100/nonfraud_num_trigram, fraud_trigram[k]*100/fraud_num_trigram, \n",
    "                 abs((nonfraud_trigram[k]*100/nonfraud_num_trigram)-(fraud_trigram[k]*100/fraud_num_trigram))) for k in nonfraud_trigram.keys() if k in fraud_trigram.keys()]\n",
    "diff_trigram = sorted(diff_trigram, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "print(nonfraud_trigram_top5)\n",
    "print(fraud_trigram_top5)\n",
    "print(diff_trigram[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer - Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X_train_cv = count_vectorizer.fit_transform(X_train['full_text'])\n",
    "X_test_cv = count_vectorizer.transform(X_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.7975206611570248\n",
      "SVM F1 Score: 0.6157760814249365\n",
      "Decision Tree F1 Score: 0.7735470941883769\n",
      "Random Forest F1 Score: 0.7353629976580797\n",
      "XGBoost F1 Score: 0.810344827586207\n",
      "average Score: 0.7465103324029249\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "logreg_model = LogisticRegression(max_iter= 1500)\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_f1 = train_and_evaluate_model(dt_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"Decision Tree F1 Score: {dt_f1}\")\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")\n",
    "\n",
    "print(f\"average Score: {np.mean([logreg_f1, svm_f1, dt_f1, rf_f1, xgb_f1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X_train_bicv = count_vectorizer.fit_transform(X_train['full_text'])\n",
    "X_test_bicv = count_vectorizer.transform(X_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.7937915742793792\n",
      "SVM F1 Score: 0.671604938271605\n",
      "Decision Tree F1 Score: 0.8023715415019763\n",
      "Random Forest F1 Score: 0.7692307692307694\n",
      "XGBoost F1 Score: 0.8008752735229759\n",
      "average Score: 0.7675748193613411\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_f1 = train_and_evaluate_model(dt_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"Decision Tree F1 Score: {dt_f1}\")\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")\n",
    "\n",
    "print(f\"average Score: {np.mean([logreg_f1, svm_f1, dt_f1, rf_f1, xgb_f1])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using text features only</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| | Logistic Regression | SVM | Decision Tree | Random Forest | XGBoost | Average |\n",
    "|----------|----------|----------|----------|----------|----------|----------|\n",
    "| TFIDF | 0.552 | 0.713 | 0.746 | 0.751 | 0.785 | 0.709 |\n",
    "| Word2Vec | 0.327 | 0.455 | 0.439 | 0.532 | 0.634 | 0.477 |\n",
    "| CountVectorizer - Unigram | 0.798 | 0.616 | 0.774 | 0.735 | 0.810 | 0.747 |\n",
    "| CountVectorizer - Bigram | 0.794 | 0.672 | 0.802 | 0.771 | 0.801 | 0.768 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, the best word embedding method to use is CountVectorizer - Bigram that obtained the highest average F1 score of 0.761 across all models. Hence, we will be using CountVectorizer - Bigram moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 50, 50)            500000    \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 200)               120800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 621001 (2.37 MB)\n",
      "Trainable params: 621001 (2.37 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "386/386 [==============================] - 29s 62ms/step - loss: 0.1458 - accuracy: 0.9637 - val_loss: 0.1024 - val_accuracy: 0.9696\n",
      "Epoch 2/10\n",
      "386/386 [==============================] - 22s 57ms/step - loss: 0.0560 - accuracy: 0.9823 - val_loss: 0.1063 - val_accuracy: 0.9718\n",
      "Epoch 3/10\n",
      "386/386 [==============================] - 22s 57ms/step - loss: 0.0236 - accuracy: 0.9922 - val_loss: 0.1324 - val_accuracy: 0.9732\n",
      "Epoch 4/10\n",
      "386/386 [==============================] - 22s 58ms/step - loss: 0.0078 - accuracy: 0.9980 - val_loss: 0.1373 - val_accuracy: 0.9735\n",
      "Epoch 5/10\n",
      "386/386 [==============================] - 22s 57ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.1747 - val_accuracy: 0.9739\n",
      "Epoch 6/10\n",
      "386/386 [==============================] - 22s 57ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.1377 - val_accuracy: 0.9741\n",
      "Epoch 7/10\n",
      "386/386 [==============================] - 22s 57ms/step - loss: 9.6649e-04 - accuracy: 0.9999 - val_loss: 0.2633 - val_accuracy: 0.9741\n",
      "Epoch 8/10\n",
      "386/386 [==============================] - 22s 57ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.2231 - val_accuracy: 0.9735\n",
      "Epoch 9/10\n",
      "386/386 [==============================] - 22s 57ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.1763 - val_accuracy: 0.9722\n",
      "Epoch 10/10\n",
      "386/386 [==============================] - 22s 57ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.1661 - val_accuracy: 0.9707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7febf545bcd0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size=10000\n",
    "corpus = data[\"full_text\"]\n",
    "onehot_repr=[one_hot(words,voc_size)for words in corpus] \n",
    "# onehot_repr[1]\n",
    "sent_length=50\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "# print(embedded_docs)\n",
    "embedding_vector_features=50\n",
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
    "model.add(Bidirectional(LSTM(100))) \n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(embedded_docs, data[\"fraudulent\"], test_size=0.3, random_state=0)\n",
    "model.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=32, validation_data=(X_test_lstm, y_test_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "# loss, accuracy = model.evaluate(X_test_lstm, y_test_lstm)\n",
    "\n",
    "# print(f\"Test Accuracy: {accuracy}\")\n",
    "# print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# Get classification report\n",
    "# report = classification_report(y_test_lstm, y_pred.round(),target_names = ['0','1'])\n",
    "# print(\"Classification Report:\")\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 3s 14ms/step\n",
      "LSTM F1 score: 0.6593406593406593\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_lstm)\n",
    "y_pred_binary = (y_pred > 0.5).astype('int32')  \n",
    "f1_test = f1_score(y_test_lstm, y_pred_binary )\n",
    "print(f'LSTM F1 score: {f1_test}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Combined text and numeric</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram and numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X_train_full[['has_questions', 'has_company_logo', 'employment_type', 'required_experience', 'required_education']]\n",
    "combined_features = hstack([\n",
    "    StandardScaler().fit_transform(numeric_features),\n",
    "    X_train_bicv])\n",
    "X_test_numeric_features = X_test_full[['has_questions', 'has_company_logo', 'employment_type', 'required_experience', 'required_education']]\n",
    "X_test_combined_features = hstack([\n",
    "    StandardScaler().fit_transform(X_test_numeric_features),\n",
    "    X_test_bicv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.8026315789473685\n",
      "SVM F1 Score: 0.6941747572815535\n",
      "Decision Tree F1 Score: 0.8346153846153846\n",
      "Random Forest F1 Score: 0.7720090293453724\n",
      "XGBoost F1 Score: 0.8197424892703862\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "    \n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_f1 = train_and_evaluate_model(dt_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"Decision Tree F1 Score: {dt_f1}\")\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 50, 50)               500000    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 200)                  120800    ['embedding_1[0][0]']         \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 200)                  0         ['bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 5)]                  0         []                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 205)                  0         ['dropout_1[0][0]',           \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 64)                   13184     ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1)                    65        ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 634049 (2.42 MB)\n",
      "Trainable params: 634049 (2.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "386/386 [==============================] - 31s 65ms/step - loss: 0.1323 - accuracy: 0.9638 - val_loss: 0.0999 - val_accuracy: 0.9692\n",
      "Epoch 2/10\n",
      "386/386 [==============================] - 25s 65ms/step - loss: 0.0419 - accuracy: 0.9864 - val_loss: 0.1107 - val_accuracy: 0.9752\n",
      "Epoch 3/10\n",
      "386/386 [==============================] - 25s 64ms/step - loss: 0.0125 - accuracy: 0.9965 - val_loss: 0.1081 - val_accuracy: 0.9681\n",
      "Epoch 4/10\n",
      "386/386 [==============================] - 26s 67ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.1429 - val_accuracy: 0.9730\n",
      "Epoch 5/10\n",
      "386/386 [==============================] - 25s 64ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.1772 - val_accuracy: 0.9747\n",
      "Epoch 6/10\n",
      "386/386 [==============================] - 27s 70ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.1824 - val_accuracy: 0.9749\n",
      "Epoch 7/10\n",
      "386/386 [==============================] - 25s 65ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.2122 - val_accuracy: 0.9747\n",
      "Epoch 8/10\n",
      "386/386 [==============================] - 27s 69ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.1875 - val_accuracy: 0.9705\n",
      "Epoch 9/10\n",
      "386/386 [==============================] - 27s 70ms/step - loss: 0.0033 - accuracy: 0.9985 - val_loss: 0.1717 - val_accuracy: 0.9717\n",
      "Epoch 10/10\n",
      "386/386 [==============================] - 28s 73ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.2221 - val_accuracy: 0.9717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fec7fd06400>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from tensorflow.keras.layers import Input, Concatenate\n",
    "# from tensorflow.keras.models import Model\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "numerical_data = data[['has_questions', 'has_company_logo', 'employment_type', 'required_experience', 'required_education']].values\n",
    "\n",
    "# Define text input\n",
    "text_input = Input(shape=(sent_length,))\n",
    "embedding_vector_features = 50\n",
    "text_embedding = Embedding(voc_size, embedding_vector_features, input_length=sent_length)(text_input)\n",
    "text_lstm = Bidirectional(LSTM(100))(text_embedding)\n",
    "text_dropout = Dropout(0.3)(text_lstm)\n",
    "\n",
    "# Define numerical input\n",
    "numerical_input = Input(shape=(numerical_data.shape[1],))\n",
    "\n",
    "# Concatenate text and numerical inputs\n",
    "concatenated = Concatenate()([text_dropout, numerical_input])\n",
    "\n",
    "# Dense layers for the merged inputs\n",
    "dense_layer = Dense(64, activation='relu')(concatenated)\n",
    "output_layer = Dense(1, activation='sigmoid')(dense_layer)\n",
    "\n",
    "# Create and compile the model\n",
    "model = Model(inputs=[text_input, numerical_input], outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#embedded_docs = np.array(embedded_docs)\n",
    "#numerical_data = np.array(numerical_data)\n",
    "labels = np.array(data[\"fraudulent\"])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "text_train, text_test, num_train, num_test, labels_train, labels_test = train_test_split(\n",
    "    embedded_docs, numerical_data, labels, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "# Train the model using both text and numerical data\n",
    "model.fit([text_train, num_train], labels_train, epochs=10, batch_size=32, validation_data=([text_test, num_test], labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 2s 13ms/step\n",
      "LSTM F1 score: 0.6875\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "test_predictions = model.predict([text_test, num_test])\n",
    "y_pred_binary = (test_predictions > 0.5).astype('int32') \n",
    "f1_test = f1_score(y_test_lstm, y_pred_binary )\n",
    "print(f'LSTM F1 score: {f1_test}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Combined features</h3>\n",
    "\n",
    "|  | Logistic Regression | SVM | Decision Tree| Random Forest | XGBoost | LSTM |\n",
    "|----------|----------|----------|----------|----------|----------|----------|\n",
    "| Categorical and Numerical Only | 0.329 | 0.00735| 0.609 | 0.548 | 0.623 |\n",
    "| Text Only | 0.794 | 0.672 | 0.811 | 0.771 | 0.801 | 0.659 |\n",
    "| Text and Numeric | 0.803 | 0.694 | 0.835 | 0.772 | 0.820 | 0.688 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
