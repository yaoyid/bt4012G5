{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('preprocessed_data_with_numerical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['full_text'] = data['title'] + \" \" + data['location']  + \" \" + data['department']  + \" \" + data['company_profile']  + \" \" + data['description']  + \" \" + data['requirements']  + \" \"  + data['benefits'] + data['industry']  + \" \" + data['function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>company_profile</th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "      <th>benefits</th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>required_experience</th>\n",
       "      <th>required_education</th>\n",
       "      <th>industry</th>\n",
       "      <th>function</th>\n",
       "      <th>fraudulent</th>\n",
       "      <th>in_balanced_dataset</th>\n",
       "      <th>missing company profile</th>\n",
       "      <th>missing company information</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marketing intern</td>\n",
       "      <td>usa ny new york</td>\n",
       "      <td>marketing</td>\n",
       "      <td>food52 created groundbreaking award winning co...</td>\n",
       "      <td>food52 fast growing james beard award winning ...</td>\n",
       "      <td>experience content management system major plu...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td>marketing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>marketing intern usa ny new york marketing foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>customer service cloud video production</td>\n",
       "      <td>nz auckland</td>\n",
       "      <td>success</td>\n",
       "      <td>90 second world cloud video production service...</td>\n",
       "      <td>organised focused vibrant awesome passion cust...</td>\n",
       "      <td>expect key responsibility communicate client 9...</td>\n",
       "      <td>get u part 90 second team gain experience work...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>marketing advertising</td>\n",
       "      <td>customer service</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>customer service cloud video production nz auc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>commissioning machinery assistant cma</td>\n",
       "      <td>usa ia wever</td>\n",
       "      <td></td>\n",
       "      <td>valor service provides workforce solution meet...</td>\n",
       "      <td>client located houston actively seeking experi...</td>\n",
       "      <td>implement pre commissioning commissioning proc...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>commissioning machinery assistant cma usa ia w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>account executive washington dc</td>\n",
       "      <td>usa dc washington</td>\n",
       "      <td>sale</td>\n",
       "      <td>passion improving quality life geography heart...</td>\n",
       "      <td>company esri environmental system research ins...</td>\n",
       "      <td>education bachelor master gi business administ...</td>\n",
       "      <td>culture anything corporate collaborative creat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>computer software</td>\n",
       "      <td>sale</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>account executive washington dc usa dc washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bill review manager</td>\n",
       "      <td>usa fl fort worth</td>\n",
       "      <td></td>\n",
       "      <td>spotsource solution llc global human capital m...</td>\n",
       "      <td>job title itemization review manager location ...</td>\n",
       "      <td>qualification rn license state texas diploma b...</td>\n",
       "      <td>full benefit offered</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>hospital health care</td>\n",
       "      <td>health care provider</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>bill review manager usa fl fort worth  spotsou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17875</th>\n",
       "      <td>account director distribution</td>\n",
       "      <td>ca toronto</td>\n",
       "      <td>sale</td>\n",
       "      <td>vend looking awesome new talent come join u wo...</td>\n",
       "      <td>case first time visited website vend award win...</td>\n",
       "      <td>ace role eat comprehensive statement work brea...</td>\n",
       "      <td>expect u open culture openly share result inpu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>computer software</td>\n",
       "      <td>sale</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>account director distribution ca toronto sale ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17876</th>\n",
       "      <td>payroll accountant</td>\n",
       "      <td>usa pa philadelphia</td>\n",
       "      <td>accounting</td>\n",
       "      <td>weblinc e commerce platform service provider f...</td>\n",
       "      <td>payroll accountant focus primarily payroll fun...</td>\n",
       "      <td>b b accounting desire fun love genuine passion...</td>\n",
       "      <td>health wellness medical plan prescription drug...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>internet</td>\n",
       "      <td>accounting auditing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>payroll accountant usa pa philadelphia account...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17877</th>\n",
       "      <td>project cost control staff engineer cost contr...</td>\n",
       "      <td>usa tx houston</td>\n",
       "      <td></td>\n",
       "      <td>provide full time permanent position many medi...</td>\n",
       "      <td>experienced project cost control staff enginee...</td>\n",
       "      <td>least 12 year professional experience ability ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>project cost control staff engineer cost contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17878</th>\n",
       "      <td>graphic designer</td>\n",
       "      <td>ng la lagos</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nemsia studio looking experienced visual graph...</td>\n",
       "      <td>1 must fluent latest version corel adobe cc es...</td>\n",
       "      <td>competitive salary compensation based experien...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>graphic design</td>\n",
       "      <td>design</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>graphic designer ng la lagos   nemsia studio l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17879</th>\n",
       "      <td>web application developer</td>\n",
       "      <td>nz n wellington</td>\n",
       "      <td>engineering</td>\n",
       "      <td>vend looking awesome new talent come join u wo...</td>\n",
       "      <td>vend award winning web based point sale softwa...</td>\n",
       "      <td>want hear depth understanding oo programming t...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>computer software</td>\n",
       "      <td>engineering</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>web application developer nz n wellington engi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17880 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title             location  \\\n",
       "0                                       marketing intern      usa ny new york   \n",
       "1                customer service cloud video production          nz auckland   \n",
       "2                  commissioning machinery assistant cma         usa ia wever   \n",
       "3                        account executive washington dc    usa dc washington   \n",
       "4                                    bill review manager    usa fl fort worth   \n",
       "...                                                  ...                  ...   \n",
       "17875                      account director distribution           ca toronto   \n",
       "17876                                 payroll accountant  usa pa philadelphia   \n",
       "17877  project cost control staff engineer cost contr...       usa tx houston   \n",
       "17878                                   graphic designer          ng la lagos   \n",
       "17879                          web application developer      nz n wellington   \n",
       "\n",
       "        department                                    company_profile  \\\n",
       "0        marketing  food52 created groundbreaking award winning co...   \n",
       "1          success  90 second world cloud video production service...   \n",
       "2                   valor service provides workforce solution meet...   \n",
       "3             sale  passion improving quality life geography heart...   \n",
       "4                   spotsource solution llc global human capital m...   \n",
       "...            ...                                                ...   \n",
       "17875         sale  vend looking awesome new talent come join u wo...   \n",
       "17876   accounting  weblinc e commerce platform service provider f...   \n",
       "17877               provide full time permanent position many medi...   \n",
       "17878                                                                   \n",
       "17879  engineering  vend looking awesome new talent come join u wo...   \n",
       "\n",
       "                                             description  \\\n",
       "0      food52 fast growing james beard award winning ...   \n",
       "1      organised focused vibrant awesome passion cust...   \n",
       "2      client located houston actively seeking experi...   \n",
       "3      company esri environmental system research ins...   \n",
       "4      job title itemization review manager location ...   \n",
       "...                                                  ...   \n",
       "17875  case first time visited website vend award win...   \n",
       "17876  payroll accountant focus primarily payroll fun...   \n",
       "17877  experienced project cost control staff enginee...   \n",
       "17878  nemsia studio looking experienced visual graph...   \n",
       "17879  vend award winning web based point sale softwa...   \n",
       "\n",
       "                                            requirements  \\\n",
       "0      experience content management system major plu...   \n",
       "1      expect key responsibility communicate client 9...   \n",
       "2      implement pre commissioning commissioning proc...   \n",
       "3      education bachelor master gi business administ...   \n",
       "4      qualification rn license state texas diploma b...   \n",
       "...                                                  ...   \n",
       "17875  ace role eat comprehensive statement work brea...   \n",
       "17876  b b accounting desire fun love genuine passion...   \n",
       "17877  least 12 year professional experience ability ...   \n",
       "17878  1 must fluent latest version corel adobe cc es...   \n",
       "17879  want hear depth understanding oo programming t...   \n",
       "\n",
       "                                                benefits  telecommuting  \\\n",
       "0                                                                     0   \n",
       "1      get u part 90 second team gain experience work...              0   \n",
       "2                                                                     0   \n",
       "3      culture anything corporate collaborative creat...              0   \n",
       "4                                   full benefit offered              0   \n",
       "...                                                  ...            ...   \n",
       "17875  expect u open culture openly share result inpu...              0   \n",
       "17876  health wellness medical plan prescription drug...              0   \n",
       "17877                                                                 0   \n",
       "17878  competitive salary compensation based experien...              0   \n",
       "17879                                                                 0   \n",
       "\n",
       "       has_company_logo  has_questions  employment_type  required_experience  \\\n",
       "0                     1              0                2                    4   \n",
       "1                     1              0                1                    6   \n",
       "2                     1              0                2                    6   \n",
       "3                     1              0                1                    5   \n",
       "4                     1              1                1                    5   \n",
       "...                 ...            ...              ...                  ...   \n",
       "17875                 1              1                1                    5   \n",
       "17876                 1              1                1                    5   \n",
       "17877                 0              0                1                    6   \n",
       "17878                 0              1                0                    6   \n",
       "17879                 1              1                1                    5   \n",
       "\n",
       "       required_education               industry              function  \\\n",
       "0                       9                                    marketing   \n",
       "1                       9  marketing advertising      customer service   \n",
       "2                       9                                                \n",
       "3                       1      computer software                  sale   \n",
       "4                       1   hospital health care  health care provider   \n",
       "...                   ...                    ...                   ...   \n",
       "17875                   9      computer software                  sale   \n",
       "17876                   1               internet   accounting auditing   \n",
       "17877                   9                                                \n",
       "17878                   6         graphic design                design   \n",
       "17879                   9      computer software           engineering   \n",
       "\n",
       "       fraudulent  in_balanced_dataset  missing company profile  \\\n",
       "0               0                    0                        1   \n",
       "1               0                    0                        1   \n",
       "2               0                    0                        1   \n",
       "3               0                    0                        1   \n",
       "4               0                    0                        1   \n",
       "...           ...                  ...                      ...   \n",
       "17875           0                    0                        1   \n",
       "17876           0                    0                        1   \n",
       "17877           0                    0                        1   \n",
       "17878           0                    0                        0   \n",
       "17879           0                    0                        1   \n",
       "\n",
       "       missing company information  \\\n",
       "0                                3   \n",
       "1                                3   \n",
       "2                                3   \n",
       "3                                3   \n",
       "4                                3   \n",
       "...                            ...   \n",
       "17875                            3   \n",
       "17876                            3   \n",
       "17877                            2   \n",
       "17878                            0   \n",
       "17879                            3   \n",
       "\n",
       "                                               full_text  \n",
       "0      marketing intern usa ny new york marketing foo...  \n",
       "1      customer service cloud video production nz auc...  \n",
       "2      commissioning machinery assistant cma usa ia w...  \n",
       "3      account executive washington dc usa dc washing...  \n",
       "4      bill review manager usa fl fort worth  spotsou...  \n",
       "...                                                  ...  \n",
       "17875  account director distribution ca toronto sale ...  \n",
       "17876  payroll accountant usa pa philadelphia account...  \n",
       "17877  project cost control staff engineer cost contr...  \n",
       "17878  graphic designer ng la lagos   nemsia studio l...  \n",
       "17879  web application developer nz n wellington engi...  \n",
       "\n",
       "[17880 rows x 20 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['full_text', 'has_questions', 'required_experience', 'required_education', 'missing company information']]\n",
    "# X = data['full_text']\n",
    "y = data['fraudulent']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train , y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "# from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Only Models</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['full_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.625\n",
      "SVM F1 Score: 0.8153846153846154\n",
      "Random Forest F1 Score: 0.793733681462141\n",
      "XGBoost F1 Score: 0.8226600985221675\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Step 4: Logistic Regression\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "# Step 5: Support Vector Machine (SVM)\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "# Step 6: Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "# Step 2: Tokenize the text\n",
    "tokenized_text = X_train['full_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Step 3: Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 4: Convert words to vectors\n",
    "def get_vector(word_list, model):\n",
    "    valid_words = [word for word in word_list if word in model.wv]\n",
    "    if not valid_words:\n",
    "        # If no valid words, return a vector of zeros or handle as needed\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model.wv[word] for word in valid_words], axis=0)\n",
    "\n",
    "X_train_word2vec = tokenized_text.apply(lambda x: get_vector(x, word2vec_model))\n",
    "X_test_word2vec = X_test['full_text'].apply(lambda x: get_vector(x.split(), word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natthaphonkanthawang/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.4222873900293255\n",
      "SVM F1 Score: 0.5123456790123456\n",
      "Random Forest F1 Score: 0.5928143712574849\n",
      "XGBoost F1 Score: 0.7037037037037037\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Train models and evaluate F1 score\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train.to_list(), y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test.to_list())\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Step 6: Logistic Regression\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "# Step 7: Support Vector Machine (SVM)\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "# Step 8: Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X_train_cv = count_vectorizer.fit_transform(X_train['full_text'])\n",
    "X_test_cv = count_vectorizer.transform(X_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natthaphonkanthawang/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.8009153318077803\n",
      "SVM F1 Score: 0.7272727272727272\n",
      "Random Forest F1 Score: 0.7905759162303664\n",
      "XGBoost F1 Score: 0.8382352941176471\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Step 4: Logistic Regression\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "# Step 5: Support Vector Machine (SVM)\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "# Step 6: Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using text features only</h3>\n",
    "\n",
    "|  | Logistic Regression | SVM | Random Forest | XGBoost | Average |\n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| TFIDF | 0.625 | 0.815 | 0.787 | 0.827 | 0.764 |\n",
    "| Word2Vec | 0.401 | 0.509 | 0.571 | 0.699 | 0.545 |\n",
    "| CountVectorizer | 0.801 | 0.727 | 0.794 | 0.838 | 0.790 |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, the best word embedding method to use is CountVectorizer that obtained the highest average F1 score of 0.790 across all models. Hence, we will be using CountVectorizer moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5567,\n",
       " 7249,\n",
       " 629,\n",
       " 8899,\n",
       " 2221,\n",
       " 4435,\n",
       " 7360,\n",
       " 3650,\n",
       " 5537,\n",
       " 4648,\n",
       " 9627,\n",
       " 629,\n",
       " 8899,\n",
       " 2221,\n",
       " 7249,\n",
       " 5537,\n",
       " 4648,\n",
       " 9627,\n",
       " 629,\n",
       " 8899,\n",
       " 2221,\n",
       " 7249,\n",
       " 711,\n",
       " 1264,\n",
       " 8550,\n",
       " 5410,\n",
       " 1845,\n",
       " 2711,\n",
       " 2290,\n",
       " 8899,\n",
       " 7113,\n",
       " 3582,\n",
       " 1939,\n",
       " 8347,\n",
       " 9627,\n",
       " 5537,\n",
       " 4648,\n",
       " 3992,\n",
       " 8899,\n",
       " 2221,\n",
       " 3874,\n",
       " 7916,\n",
       " 4984,\n",
       " 8973,\n",
       " 629,\n",
       " 7998,\n",
       " 543,\n",
       " 6898,\n",
       " 5537,\n",
       " 3443,\n",
       " 7617,\n",
       " 5537,\n",
       " 4648,\n",
       " 8427,\n",
       " 4834,\n",
       " 6257,\n",
       " 7240,\n",
       " 5402,\n",
       " 8521,\n",
       " 5088,\n",
       " 3032,\n",
       " 8899,\n",
       " 2221,\n",
       " 4915,\n",
       " 3508,\n",
       " 3929,\n",
       " 7199,\n",
       " 8899,\n",
       " 6230,\n",
       " 6071,\n",
       " 2290,\n",
       " 9897,\n",
       " 2031,\n",
       " 7856,\n",
       " 6662,\n",
       " 3850,\n",
       " 5366,\n",
       " 7996,\n",
       " 8899,\n",
       " 516,\n",
       " 6511,\n",
       " 6482,\n",
       " 4984,\n",
       " 2417,\n",
       " 2221,\n",
       " 3650,\n",
       " 2898,\n",
       " 1512,\n",
       " 6482,\n",
       " 5537,\n",
       " 4648,\n",
       " 1292,\n",
       " 6392,\n",
       " 3650,\n",
       " 6749,\n",
       " 5537,\n",
       " 4648,\n",
       " 1939,\n",
       " 7412,\n",
       " 9114,\n",
       " 5366,\n",
       " 8899,\n",
       " 5424,\n",
       " 6482,\n",
       " 6713,\n",
       " 7856,\n",
       " 1264,\n",
       " 5501,\n",
       " 9627,\n",
       " 7703,\n",
       " 5501,\n",
       " 321,\n",
       " 7577,\n",
       " 7723,\n",
       " 9426,\n",
       " 7070,\n",
       " 5346,\n",
       " 7360,\n",
       " 8230,\n",
       " 8280,\n",
       " 6324,\n",
       " 2492,\n",
       " 6898,\n",
       " 5537,\n",
       " 3443,\n",
       " 7617,\n",
       " 6898,\n",
       " 5537,\n",
       " 3443,\n",
       " 6639,\n",
       " 6898,\n",
       " 5537,\n",
       " 3443,\n",
       " 3450,\n",
       " 3178,\n",
       " 485,\n",
       " 7212,\n",
       " 9857,\n",
       " 5502,\n",
       " 5567,\n",
       " 7249,\n",
       " 4441,\n",
       " 6181,\n",
       " 6929,\n",
       " 8537,\n",
       " 9557,\n",
       " 9527,\n",
       " 3762,\n",
       " 885,\n",
       " 5616,\n",
       " 6122,\n",
       " 5094,\n",
       " 3778,\n",
       " 5480,\n",
       " 4901,\n",
       " 629,\n",
       " 8899,\n",
       " 2221,\n",
       " 7249,\n",
       " 2239,\n",
       " 9615,\n",
       " 6348,\n",
       " 3330,\n",
       " 7556,\n",
       " 7468,\n",
       " 8996,\n",
       " 5259,\n",
       " 9627,\n",
       " 4672,\n",
       " 3397,\n",
       " 4774,\n",
       " 5567,\n",
       " 7249,\n",
       " 5775,\n",
       " 7039,\n",
       " 8481,\n",
       " 8314,\n",
       " 8605,\n",
       " 6230,\n",
       " 7390,\n",
       " 2657,\n",
       " 8120,\n",
       " 6829,\n",
       " 8899,\n",
       " 6230,\n",
       " 2657,\n",
       " 3041,\n",
       " 6865,\n",
       " 4749,\n",
       " 2221,\n",
       " 637,\n",
       " 8950,\n",
       " 5204,\n",
       " 1264,\n",
       " 5746,\n",
       " 4152,\n",
       " 7856,\n",
       " 2898,\n",
       " 2599,\n",
       " 4435,\n",
       " 2453,\n",
       " 1031,\n",
       " 6386,\n",
       " 1782,\n",
       " 1833,\n",
       " 8581,\n",
       " 8481,\n",
       " 2031,\n",
       " 2113,\n",
       " 4186,\n",
       " 6095,\n",
       " 8510,\n",
       " 3358,\n",
       " 1221,\n",
       " 5854,\n",
       " 2299,\n",
       " 6386,\n",
       " 8481,\n",
       " 5854,\n",
       " 8448,\n",
       " 5537,\n",
       " 4648,\n",
       " 9627,\n",
       " 629,\n",
       " 8899,\n",
       " 2221,\n",
       " 7249,\n",
       " 6898,\n",
       " 5537,\n",
       " 3443,\n",
       " 7617,\n",
       " 5537,\n",
       " 4648,\n",
       " 9627,\n",
       " 629,\n",
       " 8899,\n",
       " 2221,\n",
       " 7249,\n",
       " 711,\n",
       " 1264,\n",
       " 8550,\n",
       " 5410,\n",
       " 1845,\n",
       " 2711,\n",
       " 2290,\n",
       " 8899,\n",
       " 7113,\n",
       " 3582,\n",
       " 1939,\n",
       " 8347,\n",
       " 9627,\n",
       " 3874,\n",
       " 7916,\n",
       " 4984,\n",
       " 8973,\n",
       " 629,\n",
       " 7998,\n",
       " 543,\n",
       " 5537,\n",
       " 4648,\n",
       " 8427,\n",
       " 4834,\n",
       " 6257,\n",
       " 7240,\n",
       " 5402,\n",
       " 8521,\n",
       " 5088,\n",
       " 3032,\n",
       " 8899,\n",
       " 2221,\n",
       " 4915,\n",
       " 3508,\n",
       " 3929,\n",
       " 7199,\n",
       " 8899,\n",
       " 6230,\n",
       " 6071,\n",
       " 2290,\n",
       " 9897,\n",
       " 2031,\n",
       " 6662,\n",
       " 3850,\n",
       " 5366,\n",
       " 7996,\n",
       " 8899,\n",
       " 516,\n",
       " 6511,\n",
       " 6482,\n",
       " 2417,\n",
       " 2221,\n",
       " 3650,\n",
       " 2898,\n",
       " 1512,\n",
       " 6482,\n",
       " 389,\n",
       " 8899,\n",
       " 6230,\n",
       " 3650,\n",
       " 6392,\n",
       " 1966,\n",
       " 2891,\n",
       " 5416,\n",
       " 2242,\n",
       " 7405,\n",
       " 5850,\n",
       " 5537,\n",
       " 4648,\n",
       " 1939,\n",
       " 7412,\n",
       " 9114,\n",
       " 5366,\n",
       " 8899,\n",
       " 5424,\n",
       " 6482,\n",
       " 6713,\n",
       " 7856,\n",
       " 1264,\n",
       " 5501,\n",
       " 9627,\n",
       " 7703,\n",
       " 5501,\n",
       " 321,\n",
       " 7577,\n",
       " 7723,\n",
       " 9426,\n",
       " 7070,\n",
       " 5346,\n",
       " 7360,\n",
       " 8230,\n",
       " 8280,\n",
       " 6324,\n",
       " 2492,\n",
       " 7360,\n",
       " 5346,\n",
       " 5202,\n",
       " 5854,\n",
       " 4343,\n",
       " 476,\n",
       " 45,\n",
       " 6076,\n",
       " 630,\n",
       " 7975,\n",
       " 8844,\n",
       " 550,\n",
       " 3448,\n",
       " 2927,\n",
       " 3041,\n",
       " 5537,\n",
       " 4648,\n",
       " 2898,\n",
       " 1192,\n",
       " 4377,\n",
       " 9772,\n",
       " 8899,\n",
       " 2221,\n",
       " 637,\n",
       " 5501,\n",
       " 8153,\n",
       " 801,\n",
       " 6230,\n",
       " 1192,\n",
       " 9945,\n",
       " 3508,\n",
       " 4008,\n",
       " 2290,\n",
       " 2221,\n",
       " 9527,\n",
       " 4884,\n",
       " 1165,\n",
       " 2657,\n",
       " 8899,\n",
       " 6230,\n",
       " 7367,\n",
       " 7046,\n",
       " 2439,\n",
       " 8899,\n",
       " 3041,\n",
       " 9983,\n",
       " 550,\n",
       " 2718,\n",
       " 3041,\n",
       " 485,\n",
       " 4774,\n",
       " 5567,\n",
       " 7249,\n",
       " 6865,\n",
       " 6929,\n",
       " 2290,\n",
       " 190,\n",
       " 7982,\n",
       " 1767,\n",
       " 9897,\n",
       " 311,\n",
       " 2290,\n",
       " 2030,\n",
       " 6230,\n",
       " 9527,\n",
       " 367,\n",
       " 3178,\n",
       " 2657,\n",
       " 4891,\n",
       " 6865,\n",
       " 1199,\n",
       " 6428,\n",
       " 8864,\n",
       " 1859,\n",
       " 469,\n",
       " 2973,\n",
       " 3847,\n",
       " 5502,\n",
       " 4774,\n",
       " 8950,\n",
       " 6389,\n",
       " 2439,\n",
       " 8951,\n",
       " 8436,\n",
       " 5254,\n",
       " 1847,\n",
       " 3762,\n",
       " 5591,\n",
       " 6804,\n",
       " 7998,\n",
       " 5939,\n",
       " 3115,\n",
       " 3874,\n",
       " 9351,\n",
       " 8950,\n",
       " 6447,\n",
       " 2331,\n",
       " 1966,\n",
       " 9826,\n",
       " 6920,\n",
       " 228,\n",
       " 5410,\n",
       " 4832,\n",
       " 9983,\n",
       " 5478,\n",
       " 3159,\n",
       " 9862,\n",
       " 5809,\n",
       " 3276,\n",
       " 5088,\n",
       " 7829,\n",
       " 9064,\n",
       " 3862,\n",
       " 6680,\n",
       " 4865,\n",
       " 5501,\n",
       " 159,\n",
       " 6230,\n",
       " 8137,\n",
       " 9748,\n",
       " 3871,\n",
       " 9897,\n",
       " 5537,\n",
       " 4648,\n",
       " 6869,\n",
       " 2726,\n",
       " 9897,\n",
       " 5567,\n",
       " 7249,\n",
       " 6230,\n",
       " 9527,\n",
       " 2281,\n",
       " 2115,\n",
       " 8072,\n",
       " 1391,\n",
       " 3443,\n",
       " 9170,\n",
       " 5410,\n",
       " 1157,\n",
       " 5842,\n",
       " 9609,\n",
       " 5410,\n",
       " 2299,\n",
       " 5211,\n",
       " 5537,\n",
       " 4648,\n",
       " 2898,\n",
       " 9182,\n",
       " 9897,\n",
       " 5088,\n",
       " 6230,\n",
       " 484,\n",
       " 6340,\n",
       " 9627,\n",
       " 5295,\n",
       " 1264,\n",
       " 9897,\n",
       " 5088,\n",
       " 9386,\n",
       " 3041,\n",
       " 6915,\n",
       " 4388,\n",
       " 6230,\n",
       " 1221,\n",
       " 4749,\n",
       " 8674,\n",
       " 2221,\n",
       " 9128,\n",
       " 2898,\n",
       " 1178,\n",
       " 5088,\n",
       " 881,\n",
       " 2439,\n",
       " 2898,\n",
       " 8138,\n",
       " 209,\n",
       " 5366,\n",
       " 5236,\n",
       " 5366,\n",
       " 110,\n",
       " 5847,\n",
       " 8899,\n",
       " 8899,\n",
       " 8481,\n",
       " 5842,\n",
       " 7735,\n",
       " 6540,\n",
       " 1031,\n",
       " 4893,\n",
       " 2561,\n",
       " 5847,\n",
       " 7735,\n",
       " 6540,\n",
       " 1031,\n",
       " 6005,\n",
       " 55,\n",
       " 8110,\n",
       " 5537,\n",
       " 4648,\n",
       " 4210,\n",
       " 8899,\n",
       " 7534,\n",
       " 2299,\n",
       " 9897,\n",
       " 3762,\n",
       " 9732,\n",
       " 796,\n",
       " 5847,\n",
       " 793,\n",
       " 3531,\n",
       " 999,\n",
       " 933,\n",
       " 8899,\n",
       " 2907,\n",
       " 8899,\n",
       " 2686,\n",
       " 6692,\n",
       " 7834,\n",
       " 7731,\n",
       " 8503,\n",
       " 2068,\n",
       " 412,\n",
       " 4056,\n",
       " 2190,\n",
       " 5202,\n",
       " 931,\n",
       " 4073,\n",
       " 8899,\n",
       " 4685,\n",
       " 3777,\n",
       " 6732,\n",
       " 5435,\n",
       " 6973,\n",
       " 8476,\n",
       " 280,\n",
       " 3671,\n",
       " 8475,\n",
       " 6569,\n",
       " 7152,\n",
       " 2819,\n",
       " 4494,\n",
       " 6191,\n",
       " 8899,\n",
       " 56,\n",
       " 6005,\n",
       " 5847,\n",
       " 3757,\n",
       " 4808,\n",
       " 280,\n",
       " 4702,\n",
       " 3676,\n",
       " 2734,\n",
       " 4576,\n",
       " 739,\n",
       " 8899,\n",
       " 5410,\n",
       " 7655,\n",
       " 2561,\n",
       " 5847,\n",
       " 2466,\n",
       " 5701,\n",
       " 6981,\n",
       " 4395,\n",
       " 5567,\n",
       " 7249]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size=10000\n",
    "corpus = data[\"full_text\"]\n",
    "onehot_repr=[one_hot(words,voc_size)for words in corpus] \n",
    "onehot_repr[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7742  178 4774 ... 4210 6447 1565]\n",
      " [2686 6692 7834 ... 4395 5567 7249]\n",
      " [7260 3753 6451 ... 5728 6230 4186]\n",
      " ...\n",
      " [7897 6156 8448 ... 7030 3443 3657]\n",
      " [9064 5609 7278 ... 3266 5609 5609]\n",
      " [7648 9945 3876 ... 7982 2030 1795]]\n"
     ]
    }
   ],
   "source": [
    "sent_length=50\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 50, 50)            500000    \n",
      "                                                                 \n",
      " bidirectional_16 (Bidirecti  (None, 200)              120800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 621,001\n",
      "Trainable params: 621,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_features=50\n",
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
    "model.add(Bidirectional(LSTM(100))) ##Just add bidirectional!!, except it would just behave as normal LSTM Model\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(embedded_docs, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "392/392 [==============================] - 26s 65ms/step - loss: 0.0065 - accuracy: 0.9978 - val_loss: 0.1136 - val_accuracy: 0.9754\n",
      "Epoch 2/10\n",
      "392/392 [==============================] - 24s 60ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.1333 - val_accuracy: 0.9789\n",
      "Epoch 3/10\n",
      "392/392 [==============================] - 24s 60ms/step - loss: 0.0075 - accuracy: 0.9982 - val_loss: 0.1034 - val_accuracy: 0.9756\n",
      "Epoch 4/10\n",
      "392/392 [==============================] - 25s 63ms/step - loss: 0.0065 - accuracy: 0.9986 - val_loss: 0.1350 - val_accuracy: 0.9808\n",
      "Epoch 5/10\n",
      "392/392 [==============================] - 24s 62ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.1441 - val_accuracy: 0.9812\n",
      "Epoch 6/10\n",
      "392/392 [==============================] - 24s 61ms/step - loss: 8.4761e-04 - accuracy: 0.9998 - val_loss: 0.1464 - val_accuracy: 0.9815\n",
      "Epoch 7/10\n",
      "392/392 [==============================] - 30s 78ms/step - loss: 5.7618e-04 - accuracy: 0.9998 - val_loss: 0.1435 - val_accuracy: 0.9804\n",
      "Epoch 8/10\n",
      "392/392 [==============================] - 31s 80ms/step - loss: 9.3039e-04 - accuracy: 0.9997 - val_loss: 0.1605 - val_accuracy: 0.9810\n",
      "Epoch 9/10\n",
      "392/392 [==============================] - 29s 75ms/step - loss: 8.2820e-04 - accuracy: 0.9998 - val_loss: 0.1696 - val_accuracy: 0.9806\n",
      "Epoch 10/10\n",
      "392/392 [==============================] - 32s 81ms/step - loss: 4.7368e-04 - accuracy: 0.9998 - val_loss: 0.1787 - val_accuracy: 0.9810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb538b78730>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=32, validation_data=(X_test_lstm, y_test_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 5s 27ms/step - loss: 0.1787 - accuracy: 0.9810\n",
      "Test Accuracy: 0.9809843301773071\n",
      "Test Loss: 0.17866438627243042\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_lstm, y_test_lstm)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 5s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5137\n",
      "           1       0.89      0.63      0.74       227\n",
      "\n",
      "    accuracy                           0.98      5364\n",
      "   macro avg       0.94      0.81      0.86      5364\n",
      "weighted avg       0.98      0.98      0.98      5364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred_binary = (y_pred > 0.5).astype('int32')  \n",
    "# Get classification report\n",
    "report = classification_report(y_test_lstm, y_pred.round(),target_names = ['0','1'])\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5120,   17],\n",
       "       [  85,  142]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM F1 score: 0.7357512953367876\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_test = f1_score(y_test_lstm, y_pred_binary )\n",
    "print(f'LSTM F1 score: {f1_test}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Combined text and numeric</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf and numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['has_company_logo', 'employment_type'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/natthaphonkanthawang/Desktop/BT4012 Project/bt4012G5/andrea_modelling.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/natthaphonkanthawang/Desktop/BT4012%20Project/bt4012G5/andrea_modelling.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m numeric_features \u001b[39m=\u001b[39m X_train[[\u001b[39m'\u001b[39;49m\u001b[39mhas_company_logo\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mhas_questions\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39memployment_type\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrequired_experience\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrequired_education\u001b[39;49m\u001b[39m'\u001b[39;49m]]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/natthaphonkanthawang/Desktop/BT4012%20Project/bt4012G5/andrea_modelling.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m combined_features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack([\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/natthaphonkanthawang/Desktop/BT4012%20Project/bt4012G5/andrea_modelling.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     StandardScaler()\u001b[39m.\u001b[39mfit_transform(numeric_features),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/natthaphonkanthawang/Desktop/BT4012%20Project/bt4012G5/andrea_modelling.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     X_train_tfidf\u001b[39m.\u001b[39mtoarray()])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/natthaphonkanthawang/Desktop/BT4012%20Project/bt4012G5/andrea_modelling.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m X_test_numeric_features \u001b[39m=\u001b[39m X_test[[\u001b[39m'\u001b[39m\u001b[39mhas_company_logo\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhas_questions\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39memployment_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrequired_experience\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrequired_education\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/indexes/base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 6133\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['has_company_logo', 'employment_type'] not in index\""
     ]
    }
   ],
   "source": [
    "numeric_features = X_train[['has_company_logo', 'has_questions', 'employment_type', 'required_experience', 'required_education']]\n",
    "combined_features = np.hstack([\n",
    "    StandardScaler().fit_transform(numeric_features),\n",
    "    X_train_tfidf.toarray()])\n",
    "X_test_numeric_features = X_test[['has_company_logo', 'has_questions', 'employment_type', 'required_experience', 'required_education']]\n",
    "X_test_combined_features = np.hstack([\n",
    "    StandardScaler().fit_transform(X_test_numeric_features),\n",
    "    X_test_tfidf.toarray()])\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "# Step 5: Support Vector Machine (SVM)\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "# Step 6: Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word2vec_array = np.vstack(X_train_word2vec.apply(lambda x: x.flatten()).to_numpy())\n",
    "test_word2vec_array = np.vstack(X_test_word2vec.apply(lambda x: x.flatten()).to_numpy())\n",
    "combined_features = np.hstack([\n",
    "    StandardScaler().fit_transform(numeric_features),\n",
    "    train_word2vec_array])\n",
    "X_test_numeric_features = X_test[['has_company_logo', 'has_questions', 'employment_type', 'required_experience', 'required_education']]\n",
    "X_test_combined_features = np.hstack([\n",
    "    StandardScaler().fit_transform(X_test_numeric_features),\n",
    "    test_word2vec_array])\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "# Step 5: Support Vector Machine (SVM)\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "# Step 6: Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X_train[['has_questions', 'required_experience', 'required_education', 'missing company information']]\n",
    "combined_features = np.hstack([\n",
    "    StandardScaler().fit_transform(numeric_features),\n",
    "    X_train_cv.toarray()])\n",
    "X_test_numeric_features = X_test[['has_questions', 'required_experience', 'required_education', 'missing company information']]\n",
    "X_test_combined_features = np.hstack([\n",
    "    StandardScaler().fit_transform(X_test_numeric_features),\n",
    "    X_test_cv.toarray()])\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "# Step 5: Support Vector Machine (SVM)\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "# Step 6: Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Combined features</h3>\n",
    "\n",
    "|  | Logistic Regression | SVM | Random Forest | XGBoost |\n",
    "|----------|----------|----------|----------|----------|\n",
    "| Text Only | 0.801 | 0.727 | 0.794 | 0.838 |\n",
    "| Text and Numeric | 0.823 | 0.729 | 0.791 | 0.846 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define and compile the neural network model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred_valid = model.predict(X_val)\n",
    "y_pred_valid = (y_pred_valid > 0.5).astype(int)  # Convert probabilities to binary labels\n",
    "\n",
    "# Calculate the F1 score on the validation set\n",
    "f1_valid = f1_score(y_val, y_pred_valid)\n",
    "\n",
    "print(\"Validation Set F1 Score:\")\n",
    "print(f\"F1 Score: {f1_valid}\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_test = (y_pred_test > 0.5).astype(int)  # Convert probabilities to binary labels\n",
    "\n",
    "# Calculate the F1 score on the test set\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Test Set F1 Score:\")\n",
    "print(f\"F1 Score: {f1_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
