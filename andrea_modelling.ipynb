{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense,Bidirectional\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "# from tensorflow.keras.preprocessing.text import one_hot\n",
    "# from tensorflow.keras.layers import Input, Concatenate\n",
    "# from tensorflow.keras.models import Model\n",
    "# from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessed_data_with_numerical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>company_profile</th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "      <th>benefits</th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>required_experience</th>\n",
       "      <th>required_education</th>\n",
       "      <th>industry</th>\n",
       "      <th>function</th>\n",
       "      <th>fraudulent</th>\n",
       "      <th>in_balanced_dataset</th>\n",
       "      <th>missing company profile</th>\n",
       "      <th>missing company information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marketing intern</td>\n",
       "      <td>usa ny new york</td>\n",
       "      <td>marketing</td>\n",
       "      <td>food52 created groundbreaking award winning co...</td>\n",
       "      <td>food52 fast growing james beard award winning ...</td>\n",
       "      <td>experience content management system major plu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>marketing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>customer service cloud video production</td>\n",
       "      <td>nz auckland</td>\n",
       "      <td>success</td>\n",
       "      <td>90 second world cloud video production service...</td>\n",
       "      <td>organised focused vibrant awesome passion cust...</td>\n",
       "      <td>expect key responsibility communicate client 9...</td>\n",
       "      <td>get u part 90 second team gain experience work...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>marketing advertising</td>\n",
       "      <td>customer service</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>commissioning machinery assistant cma</td>\n",
       "      <td>usa ia wever</td>\n",
       "      <td>NaN</td>\n",
       "      <td>valor service provides workforce solution meet...</td>\n",
       "      <td>client located houston actively seeking experi...</td>\n",
       "      <td>implement pre commissioning commissioning proc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>account executive washington dc</td>\n",
       "      <td>usa dc washington</td>\n",
       "      <td>sale</td>\n",
       "      <td>passion improving quality life geography heart...</td>\n",
       "      <td>company esri environmental system research ins...</td>\n",
       "      <td>education bachelor master gi business administ...</td>\n",
       "      <td>culture anything corporate collaborative creat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>computer software</td>\n",
       "      <td>sale</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bill review manager</td>\n",
       "      <td>usa fl fort worth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spotsource solution llc global human capital m...</td>\n",
       "      <td>job title itemization review manager location ...</td>\n",
       "      <td>qualification rn license state texas diploma b...</td>\n",
       "      <td>full benefit offered</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>hospital health care</td>\n",
       "      <td>health care provider</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17875</th>\n",
       "      <td>account director distribution</td>\n",
       "      <td>ca toronto</td>\n",
       "      <td>sale</td>\n",
       "      <td>vend looking awesome new talent come join u wo...</td>\n",
       "      <td>case first time visited website vend award win...</td>\n",
       "      <td>ace role eat comprehensive statement work brea...</td>\n",
       "      <td>expect u open culture openly share result inpu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>computer software</td>\n",
       "      <td>sale</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17876</th>\n",
       "      <td>payroll accountant</td>\n",
       "      <td>usa pa philadelphia</td>\n",
       "      <td>accounting</td>\n",
       "      <td>weblinc e commerce platform service provider f...</td>\n",
       "      <td>payroll accountant focus primarily payroll fun...</td>\n",
       "      <td>b b accounting desire fun love genuine passion...</td>\n",
       "      <td>health wellness medical plan prescription drug...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>internet</td>\n",
       "      <td>accounting auditing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17877</th>\n",
       "      <td>project cost control staff engineer cost contr...</td>\n",
       "      <td>usa tx houston</td>\n",
       "      <td>NaN</td>\n",
       "      <td>provide full time permanent position many medi...</td>\n",
       "      <td>experienced project cost control staff enginee...</td>\n",
       "      <td>least 12 year professional experience ability ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17878</th>\n",
       "      <td>graphic designer</td>\n",
       "      <td>ng la lagos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nemsia studio looking experienced visual graph...</td>\n",
       "      <td>1 must fluent latest version corel adobe cc es...</td>\n",
       "      <td>competitive salary compensation based experien...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>graphic design</td>\n",
       "      <td>design</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17879</th>\n",
       "      <td>web application developer</td>\n",
       "      <td>nz n wellington</td>\n",
       "      <td>engineering</td>\n",
       "      <td>vend looking awesome new talent come join u wo...</td>\n",
       "      <td>vend award winning web based point sale softwa...</td>\n",
       "      <td>want hear depth understanding oo programming t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>computer software</td>\n",
       "      <td>engineering</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17880 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title             location  \\\n",
       "0                                       marketing intern      usa ny new york   \n",
       "1                customer service cloud video production          nz auckland   \n",
       "2                  commissioning machinery assistant cma         usa ia wever   \n",
       "3                        account executive washington dc    usa dc washington   \n",
       "4                                    bill review manager    usa fl fort worth   \n",
       "...                                                  ...                  ...   \n",
       "17875                      account director distribution           ca toronto   \n",
       "17876                                 payroll accountant  usa pa philadelphia   \n",
       "17877  project cost control staff engineer cost contr...       usa tx houston   \n",
       "17878                                   graphic designer          ng la lagos   \n",
       "17879                          web application developer      nz n wellington   \n",
       "\n",
       "        department                                    company_profile  \\\n",
       "0        marketing  food52 created groundbreaking award winning co...   \n",
       "1          success  90 second world cloud video production service...   \n",
       "2              NaN  valor service provides workforce solution meet...   \n",
       "3             sale  passion improving quality life geography heart...   \n",
       "4              NaN  spotsource solution llc global human capital m...   \n",
       "...            ...                                                ...   \n",
       "17875         sale  vend looking awesome new talent come join u wo...   \n",
       "17876   accounting  weblinc e commerce platform service provider f...   \n",
       "17877          NaN  provide full time permanent position many medi...   \n",
       "17878          NaN                                                NaN   \n",
       "17879  engineering  vend looking awesome new talent come join u wo...   \n",
       "\n",
       "                                             description  \\\n",
       "0      food52 fast growing james beard award winning ...   \n",
       "1      organised focused vibrant awesome passion cust...   \n",
       "2      client located houston actively seeking experi...   \n",
       "3      company esri environmental system research ins...   \n",
       "4      job title itemization review manager location ...   \n",
       "...                                                  ...   \n",
       "17875  case first time visited website vend award win...   \n",
       "17876  payroll accountant focus primarily payroll fun...   \n",
       "17877  experienced project cost control staff enginee...   \n",
       "17878  nemsia studio looking experienced visual graph...   \n",
       "17879  vend award winning web based point sale softwa...   \n",
       "\n",
       "                                            requirements  \\\n",
       "0      experience content management system major plu...   \n",
       "1      expect key responsibility communicate client 9...   \n",
       "2      implement pre commissioning commissioning proc...   \n",
       "3      education bachelor master gi business administ...   \n",
       "4      qualification rn license state texas diploma b...   \n",
       "...                                                  ...   \n",
       "17875  ace role eat comprehensive statement work brea...   \n",
       "17876  b b accounting desire fun love genuine passion...   \n",
       "17877  least 12 year professional experience ability ...   \n",
       "17878  1 must fluent latest version corel adobe cc es...   \n",
       "17879  want hear depth understanding oo programming t...   \n",
       "\n",
       "                                                benefits  telecommuting  \\\n",
       "0                                                    NaN              0   \n",
       "1      get u part 90 second team gain experience work...              0   \n",
       "2                                                    NaN              0   \n",
       "3      culture anything corporate collaborative creat...              0   \n",
       "4                                   full benefit offered              0   \n",
       "...                                                  ...            ...   \n",
       "17875  expect u open culture openly share result inpu...              0   \n",
       "17876  health wellness medical plan prescription drug...              0   \n",
       "17877                                                NaN              0   \n",
       "17878  competitive salary compensation based experien...              0   \n",
       "17879                                                NaN              0   \n",
       "\n",
       "       has_company_logo  has_questions  employment_type  required_experience  \\\n",
       "0                     1              0                2                    4   \n",
       "1                     1              0                1                    6   \n",
       "2                     1              0                2                    6   \n",
       "3                     1              0                1                    5   \n",
       "4                     1              1                1                    5   \n",
       "...                 ...            ...              ...                  ...   \n",
       "17875                 1              1                1                    5   \n",
       "17876                 1              1                1                    5   \n",
       "17877                 0              0                1                    6   \n",
       "17878                 0              1                0                    6   \n",
       "17879                 1              1                1                    5   \n",
       "\n",
       "       required_education               industry              function  \\\n",
       "0                       9                    NaN             marketing   \n",
       "1                       9  marketing advertising      customer service   \n",
       "2                       9                    NaN                   NaN   \n",
       "3                       1      computer software                  sale   \n",
       "4                       1   hospital health care  health care provider   \n",
       "...                   ...                    ...                   ...   \n",
       "17875                   9      computer software                  sale   \n",
       "17876                   1               internet   accounting auditing   \n",
       "17877                   9                    NaN                   NaN   \n",
       "17878                   6         graphic design                design   \n",
       "17879                   9      computer software           engineering   \n",
       "\n",
       "       fraudulent  in_balanced_dataset  missing company profile  \\\n",
       "0               0                    0                        1   \n",
       "1               0                    0                        1   \n",
       "2               0                    0                        1   \n",
       "3               0                    0                        1   \n",
       "4               0                    0                        1   \n",
       "...           ...                  ...                      ...   \n",
       "17875           0                    0                        1   \n",
       "17876           0                    0                        1   \n",
       "17877           0                    0                        1   \n",
       "17878           0                    0                        0   \n",
       "17879           0                    0                        1   \n",
       "\n",
       "       missing company information  \n",
       "0                                3  \n",
       "1                                3  \n",
       "2                                3  \n",
       "3                                3  \n",
       "4                                3  \n",
       "...                            ...  \n",
       "17875                            3  \n",
       "17876                            3  \n",
       "17877                            2  \n",
       "17878                            0  \n",
       "17879                            3  \n",
       "\n",
       "[17880 rows x 19 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN with empty strings\n",
    "data.fillna('', inplace=True)\n",
    "\n",
    "# combine all text\n",
    "data['full_text'] = data['title'] + \" \" + data['location']  + \" \" + data['department']  + \" \" + data['company_profile']  + \" \" + data['description']  + \" \" + data['requirements']  + \" \"  + data['benefits'] + data['industry']  + \" \" + data['function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test_full,y_train , y_test = train_test_split(data.drop('fraudulent', axis=1), data[\"fraudulent\"], test_size=0.3, random_state=0)\n",
    "X_train = X_train_full[['full_text', 'has_questions', 'employment_type', 'required_experience', 'required_education', 'missing company information']]\n",
    "X_test = X_test_full[['full_text', 'has_questions', 'employment_type', 'required_experience', 'required_education', 'missing company information']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Only Models</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['full_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.625\n",
      "SVM F1 Score: 0.8153846153846154\n",
      "Random Forest F1 Score: 0.7905759162303664\n",
      "XGBoost F1 Score: 0.8226600985221675\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenized_text = X_train['full_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert words to vectors\n",
    "def get_vector(word_list, model):\n",
    "    valid_words = [word for word in word_list if word in model.wv]\n",
    "    if not valid_words:\n",
    "        # If no valid words, return a vector of zeros or handle as needed\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model.wv[word] for word in valid_words], axis=0)\n",
    "\n",
    "X_train_word2vec = tokenized_text.apply(lambda x: get_vector(x, word2vec_model))\n",
    "X_test_word2vec = X_test['full_text'].apply(lambda x: get_vector(x.split(), word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreayeo/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.39644970414201186\n",
      "SVM F1 Score: 0.49844236760124616\n",
      "Random Forest F1 Score: 0.5970149253731343\n",
      "XGBoost F1 Score: 0.7068062827225131\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train.to_list(), y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test.to_list())\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngrams Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train into fraud and non-fraud\n",
    "X_train_nonfraud = X_train.loc[y_train==0]\n",
    "X_train_fraud = X_train.loc[y_train==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to return ngrams sorted by frequency\n",
    "def get_ngrams(ngram, corpus):\n",
    "    vec = CountVectorizer(ngram_range=(ngram, ngram)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = {}\n",
    "    for word, idx in vec.vocabulary_.items():\n",
    "        words_freq[word] = sum_words[0, idx]\n",
    "    words_freq = dict(sorted(words_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('php', 955), ('tidewater', 950), ('athens', 913), ('european', 808), ('1500', 727)]\n",
      "[('aker', 177), ('0fa3f7c5e23a16de16a841e368006cae916884407d90b154dfef3976483a71ae', 60), ('accion', 53), ('novation', 38), ('ddb080358fa5eecf5a67c649cfb4ffc343c484389f1bbaf2a1cb071e3f2b6e7e', 36)]\n",
      "[('team', 0.9025851637746181, 0.5926581845862268, 0.3099269791883913), ('engineering', 0.16009264936303563, 0.45206692454676795, 0.2919742751837323), ('position', 0.2662371577147788, 0.5444998524180144, 0.27826269470323556), ('skill', 0.5543066058087517, 0.7946124807755045, 0.24030587496675282), ('marketing', 0.3309554627764315, 0.10874462102499573, 0.2222108417514358)]\n"
     ]
    }
   ],
   "source": [
    "nonfraud_unigram = get_ngrams(1, X_train_nonfraud['full_text'])\n",
    "fraud_unigram = get_ngrams(1, X_train_fraud['full_text'])\n",
    "nonfraud_unigram_top5 = [(k,v) for k,v in nonfraud_unigram.items() if k not in fraud_unigram.keys()][:5]\n",
    "fraud_unigram_top5 = [(k,v) for k,v in fraud_unigram.items() if k not in nonfraud_unigram.keys()][:5]\n",
    "\n",
    "nonfraud_num_unigram, fraud_num_unigram = sum(nonfraud_unigram.values()), sum(fraud_unigram.values())\n",
    "diff_unigram = [(k, nonfraud_unigram[k]*100/nonfraud_num_unigram, fraud_unigram[k]*100/fraud_num_unigram, \n",
    "                 abs((nonfraud_unigram[k]*100/nonfraud_num_unigram)-(fraud_unigram[k]*100/fraud_num_unigram))) for k in nonfraud_unigram.keys() if k in fraud_unigram.keys()]\n",
    "diff_unigram = sorted(diff_unigram, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "print(nonfraud_unigram_top5)\n",
    "print(fraud_unigram_top5)\n",
    "print(diff_unigram[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('increase productivity', 800), ('university degree', 795), ('document communication', 788), ('degree required', 696), ('medium large', 612)]\n",
      "[('aker solution', 172), ('aptitude staffing', 88), ('bring discovery', 60), ('production maximize', 60), ('maximize recovery', 60)]\n",
      "[('data entry', 0.008304260280918472, 0.1904717297799427, 0.18216746949902424), ('oil gas', 0.012244713198530764, 0.13504757890135283, 0.12280286570282206), ('work home', 0.007457551389530705, 0.09523586488997135, 0.08777831350044064), ('gas industry', 0.005861830786530685, 0.09211337751652966, 0.08625154672999898), ('signing bonus', 9.769717977551142e-05, 0.07806218433604209, 0.07796448715626658)]\n"
     ]
    }
   ],
   "source": [
    "nonfraud_bigram = get_ngrams(2, X_train_nonfraud['full_text'])\n",
    "fraud_bigram = get_ngrams(2,  X_train_fraud['full_text'])\n",
    "nonfraud_bigram_top5 = [(k,v) for k,v in nonfraud_bigram.items() if k not in fraud_bigram.keys()][:5]\n",
    "fraud_bigram_top5 = [(k,v) for k,v in fraud_bigram.items() if k not in nonfraud_bigram.keys()][:5]\n",
    "\n",
    "nonfraud_num_bigram, fraud_num_bigram = sum(nonfraud_bigram.values()), sum(fraud_bigram.values())\n",
    "diff_bigram = [(k, nonfraud_bigram[k]*100/nonfraud_num_bigram, fraud_bigram[k]*100/fraud_num_bigram, \n",
    "                abs((nonfraud_bigram[k]*100/nonfraud_num_bigram)-(fraud_bigram[k]*100/fraud_num_bigram))) for k in nonfraud_bigram.keys() if k in fraud_bigram.keys()]\n",
    "diff_bigram = sorted(diff_bigram, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "print(nonfraud_bigram_top5)\n",
    "print(fraud_bigram_top5)\n",
    "print(diff_bigram[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('full time permanent', 590), ('time permanent position', 566), ('permanent position many', 553), ('position many medium', 553), ('many medium large', 553)]\n",
      "[('gas industry engineering', 62), ('28 000 people', 61), ('aker solution global', 60), ('solution global provider', 60), ('global provider product', 60)]\n",
      "[('oil gas industry', 0.005884591393588934, 0.09257515847611875, 0.08669056708252981), ('product system service', 6.538434881765482e-05, 0.047072114479382414, 0.04700673013056476), ('approximately 28 000', 6.538434881765482e-05, 0.047072114479382414, 0.04700673013056476), ('service oil gas', 0.00019615304645296446, 0.047072114479382414, 0.04687596143292945), ('usa tx houston', 0.004184598324329909, 0.05099479068599762, 0.04681019236166771)]\n"
     ]
    }
   ],
   "source": [
    "nonfraud_trigram = get_ngrams(3,  X_train_nonfraud['full_text'])\n",
    "fraud_trigram = get_ngrams(3,  X_train_fraud['full_text'])\n",
    "nonfraud_trigram_top5 = [(k,v) for k,v in nonfraud_trigram.items() if k not in fraud_trigram.keys()][:5]\n",
    "fraud_trigram_top5 = [(k,v) for k,v in fraud_trigram.items() if k not in nonfraud_trigram.keys()][:5]\n",
    "\n",
    "nonfraud_num_trigram, fraud_num_trigram = sum(nonfraud_trigram.values()), sum(fraud_trigram.values())\n",
    "diff_trigram = [(k, nonfraud_trigram[k]*100/nonfraud_num_trigram, fraud_trigram[k]*100/fraud_num_trigram, \n",
    "                 abs((nonfraud_trigram[k]*100/nonfraud_num_trigram)-(fraud_trigram[k]*100/fraud_num_trigram))) for k in nonfraud_trigram.keys() if k in fraud_trigram.keys()]\n",
    "diff_trigram = sorted(diff_trigram, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "print(nonfraud_trigram_top5)\n",
    "print(fraud_trigram_top5)\n",
    "print(diff_trigram[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer - Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X_train_cv = count_vectorizer.fit_transform(X_train['full_text'])\n",
    "X_test_cv = count_vectorizer.transform(X_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreayeo/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.8009153318077803\n",
      "SVM F1 Score: 0.7272727272727272\n",
      "Random Forest F1 Score: 0.7905759162303664\n",
      "XGBoost F1 Score: 0.8382352941176471\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X_train_bicv = count_vectorizer.fit_transform(X_train['full_text'])\n",
    "X_test_bicv = count_vectorizer.transform(X_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.8320802005012532\n",
      "SVM F1 Score: 0.7391304347826088\n",
      "Random Forest F1 Score: 0.8112244897959183\n",
      "XGBoost F1 Score: 0.7959183673469389\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using text features only</h3>\n",
    "\n",
    "|  | Logistic Regression | SVM | Random Forest | XGBoost | Average |\n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| TFIDF | 0.625 | 0.815 | 0.790 | 0.823 | 0.763 |\n",
    "| Word2Vec | 0.385 | 0.497 | 0.568 | 0.686 | 0.534 |\n",
    "| CountVectorizer - Unigram | 0.801 | 0.727 | 0.791 | 0.838 | 0.789 |\n",
    "| CountVectorizer - Bigram | 0.832 | 0.739 | 0.817 | 0.796 | 0.796 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, the best word embedding method to use is CountVectorizer - Bigram that obtained the highest average F1 score of 0.796 across all models. Hence, we will be using CountVectorizer - Bigram moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 11:26:54.813638: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense,Bidirectional\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "# from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1891,\n",
       " 2515,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 9489,\n",
       " 5111,\n",
       " 1215,\n",
       " 6958,\n",
       " 8934,\n",
       " 7709,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 2515,\n",
       " 6958,\n",
       " 8934,\n",
       " 7709,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 2515,\n",
       " 3866,\n",
       " 6250,\n",
       " 3559,\n",
       " 8786,\n",
       " 4364,\n",
       " 5708,\n",
       " 5569,\n",
       " 4435,\n",
       " 4993,\n",
       " 4672,\n",
       " 4437,\n",
       " 8096,\n",
       " 7709,\n",
       " 6958,\n",
       " 8934,\n",
       " 2152,\n",
       " 4435,\n",
       " 1068,\n",
       " 9063,\n",
       " 4103,\n",
       " 3359,\n",
       " 7822,\n",
       " 2552,\n",
       " 4267,\n",
       " 4360,\n",
       " 7102,\n",
       " 6958,\n",
       " 9609,\n",
       " 3947,\n",
       " 6958,\n",
       " 8934,\n",
       " 9261,\n",
       " 9255,\n",
       " 1872,\n",
       " 2307,\n",
       " 9531,\n",
       " 4751,\n",
       " 9317,\n",
       " 7664,\n",
       " 4435,\n",
       " 1068,\n",
       " 6244,\n",
       " 9318,\n",
       " 8556,\n",
       " 5286,\n",
       " 4435,\n",
       " 7225,\n",
       " 9837,\n",
       " 5569,\n",
       " 405,\n",
       " 5512,\n",
       " 74,\n",
       " 7864,\n",
       " 4512,\n",
       " 4341,\n",
       " 7098,\n",
       " 4435,\n",
       " 7436,\n",
       " 1693,\n",
       " 6288,\n",
       " 3359,\n",
       " 64,\n",
       " 1068,\n",
       " 1215,\n",
       " 1566,\n",
       " 9620,\n",
       " 6288,\n",
       " 6958,\n",
       " 8934,\n",
       " 8918,\n",
       " 6169,\n",
       " 1215,\n",
       " 3061,\n",
       " 6958,\n",
       " 8934,\n",
       " 4437,\n",
       " 3326,\n",
       " 6389,\n",
       " 4341,\n",
       " 4435,\n",
       " 8978,\n",
       " 6288,\n",
       " 4690,\n",
       " 74,\n",
       " 6250,\n",
       " 3005,\n",
       " 7709,\n",
       " 4175,\n",
       " 3005,\n",
       " 5615,\n",
       " 2579,\n",
       " 9802,\n",
       " 6418,\n",
       " 1200,\n",
       " 3225,\n",
       " 5111,\n",
       " 9147,\n",
       " 1784,\n",
       " 4529,\n",
       " 9872,\n",
       " 7102,\n",
       " 6958,\n",
       " 9609,\n",
       " 3947,\n",
       " 7102,\n",
       " 6958,\n",
       " 9609,\n",
       " 5708,\n",
       " 7102,\n",
       " 6958,\n",
       " 9609,\n",
       " 6073,\n",
       " 185,\n",
       " 9592,\n",
       " 3497,\n",
       " 4318,\n",
       " 4129,\n",
       " 1891,\n",
       " 2515,\n",
       " 1093,\n",
       " 3805,\n",
       " 9101,\n",
       " 382,\n",
       " 5070,\n",
       " 161,\n",
       " 9477,\n",
       " 5209,\n",
       " 2799,\n",
       " 8657,\n",
       " 5516,\n",
       " 62,\n",
       " 870,\n",
       " 9625,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 2515,\n",
       " 6198,\n",
       " 5591,\n",
       " 8731,\n",
       " 4600,\n",
       " 5684,\n",
       " 6189,\n",
       " 97,\n",
       " 2710,\n",
       " 7709,\n",
       " 925,\n",
       " 8899,\n",
       " 4412,\n",
       " 1891,\n",
       " 2515,\n",
       " 5047,\n",
       " 2832,\n",
       " 1430,\n",
       " 8920,\n",
       " 8772,\n",
       " 7225,\n",
       " 2185,\n",
       " 5548,\n",
       " 8560,\n",
       " 4474,\n",
       " 4435,\n",
       " 7225,\n",
       " 5548,\n",
       " 2847,\n",
       " 2266,\n",
       " 4500,\n",
       " 1068,\n",
       " 2067,\n",
       " 2142,\n",
       " 8552,\n",
       " 6250,\n",
       " 6240,\n",
       " 338,\n",
       " 74,\n",
       " 1566,\n",
       " 5630,\n",
       " 9489,\n",
       " 7312,\n",
       " 8718,\n",
       " 6696,\n",
       " 9515,\n",
       " 3872,\n",
       " 9037,\n",
       " 1430,\n",
       " 5512,\n",
       " 6999,\n",
       " 7374,\n",
       " 8263,\n",
       " 4282,\n",
       " 3049,\n",
       " 9576,\n",
       " 9206,\n",
       " 6131,\n",
       " 6696,\n",
       " 1430,\n",
       " 9206,\n",
       " 6175,\n",
       " 6958,\n",
       " 8934,\n",
       " 7709,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 2515,\n",
       " 7102,\n",
       " 6958,\n",
       " 9609,\n",
       " 3947,\n",
       " 6958,\n",
       " 8934,\n",
       " 7709,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 2515,\n",
       " 3866,\n",
       " 6250,\n",
       " 3559,\n",
       " 8786,\n",
       " 4364,\n",
       " 5708,\n",
       " 5569,\n",
       " 4435,\n",
       " 4993,\n",
       " 4672,\n",
       " 4437,\n",
       " 8096,\n",
       " 7709,\n",
       " 9063,\n",
       " 4103,\n",
       " 3359,\n",
       " 7822,\n",
       " 2552,\n",
       " 4267,\n",
       " 4360,\n",
       " 6958,\n",
       " 8934,\n",
       " 9261,\n",
       " 9255,\n",
       " 1872,\n",
       " 2307,\n",
       " 9531,\n",
       " 4751,\n",
       " 9317,\n",
       " 7664,\n",
       " 4435,\n",
       " 1068,\n",
       " 6244,\n",
       " 9318,\n",
       " 8556,\n",
       " 5286,\n",
       " 4435,\n",
       " 7225,\n",
       " 9837,\n",
       " 5569,\n",
       " 405,\n",
       " 5512,\n",
       " 7864,\n",
       " 4512,\n",
       " 4341,\n",
       " 7098,\n",
       " 4435,\n",
       " 7436,\n",
       " 1693,\n",
       " 6288,\n",
       " 64,\n",
       " 1068,\n",
       " 1215,\n",
       " 1566,\n",
       " 9620,\n",
       " 6288,\n",
       " 5277,\n",
       " 4435,\n",
       " 7225,\n",
       " 1215,\n",
       " 6169,\n",
       " 7041,\n",
       " 911,\n",
       " 8802,\n",
       " 5941,\n",
       " 499,\n",
       " 3960,\n",
       " 6958,\n",
       " 8934,\n",
       " 4437,\n",
       " 3326,\n",
       " 6389,\n",
       " 4341,\n",
       " 4435,\n",
       " 8978,\n",
       " 6288,\n",
       " 4690,\n",
       " 74,\n",
       " 6250,\n",
       " 3005,\n",
       " 7709,\n",
       " 4175,\n",
       " 3005,\n",
       " 5615,\n",
       " 2579,\n",
       " 9802,\n",
       " 6418,\n",
       " 1200,\n",
       " 3225,\n",
       " 5111,\n",
       " 9147,\n",
       " 1784,\n",
       " 4529,\n",
       " 9872,\n",
       " 5111,\n",
       " 3225,\n",
       " 5663,\n",
       " 9206,\n",
       " 3231,\n",
       " 8306,\n",
       " 6723,\n",
       " 8540,\n",
       " 3714,\n",
       " 9450,\n",
       " 7864,\n",
       " 6496,\n",
       " 5520,\n",
       " 1868,\n",
       " 2847,\n",
       " 6958,\n",
       " 8934,\n",
       " 1566,\n",
       " 9064,\n",
       " 7056,\n",
       " 1081,\n",
       " 4435,\n",
       " 1068,\n",
       " 2067,\n",
       " 3005,\n",
       " 2703,\n",
       " 3039,\n",
       " 8778,\n",
       " 9064,\n",
       " 2111,\n",
       " 9318,\n",
       " 7950,\n",
       " 5569,\n",
       " 1068,\n",
       " 161,\n",
       " 8007,\n",
       " 9563,\n",
       " 5548,\n",
       " 4435,\n",
       " 7225,\n",
       " 6856,\n",
       " 1875,\n",
       " 4873,\n",
       " 4435,\n",
       " 2847,\n",
       " 4884,\n",
       " 6496,\n",
       " 4419,\n",
       " 2847,\n",
       " 9592,\n",
       " 4412,\n",
       " 1891,\n",
       " 2515,\n",
       " 2266,\n",
       " 9101,\n",
       " 5569,\n",
       " 3031,\n",
       " 4104,\n",
       " 4574,\n",
       " 405,\n",
       " 4847,\n",
       " 5569,\n",
       " 2792,\n",
       " 7225,\n",
       " 161,\n",
       " 2129,\n",
       " 185,\n",
       " 5548,\n",
       " 5859,\n",
       " 5376,\n",
       " 5440,\n",
       " 7033,\n",
       " 2487,\n",
       " 5091,\n",
       " 8146,\n",
       " 3647,\n",
       " 6205,\n",
       " 4129,\n",
       " 4412,\n",
       " 2142,\n",
       " 6664,\n",
       " 4873,\n",
       " 5484,\n",
       " 1344,\n",
       " 6667,\n",
       " 9487,\n",
       " 9477,\n",
       " 4191,\n",
       " 6353,\n",
       " 1179,\n",
       " 4634,\n",
       " 538,\n",
       " 9063,\n",
       " 647,\n",
       " 2142,\n",
       " 3170,\n",
       " 419,\n",
       " 7041,\n",
       " 384,\n",
       " 5418,\n",
       " 9665,\n",
       " 8786,\n",
       " 5346,\n",
       " 4884,\n",
       " 9477,\n",
       " 238,\n",
       " 8415,\n",
       " 1554,\n",
       " 8252,\n",
       " 9317,\n",
       " 7151,\n",
       " 3883,\n",
       " 8359,\n",
       " 1658,\n",
       " 2412,\n",
       " 3005,\n",
       " 3226,\n",
       " 7225,\n",
       " 6740,\n",
       " 2487,\n",
       " 7948,\n",
       " 405,\n",
       " 6958,\n",
       " 8934,\n",
       " 1057,\n",
       " 1723,\n",
       " 405,\n",
       " 1891,\n",
       " 2515,\n",
       " 7225,\n",
       " 161,\n",
       " 4870,\n",
       " 7989,\n",
       " 8110,\n",
       " 6445,\n",
       " 9609,\n",
       " 7559,\n",
       " 8786,\n",
       " 2896,\n",
       " 1567,\n",
       " 8502,\n",
       " 8786,\n",
       " 2008,\n",
       " 2598,\n",
       " 6958,\n",
       " 8934,\n",
       " 1566,\n",
       " 8859,\n",
       " 405,\n",
       " 9317,\n",
       " 7225,\n",
       " 2664,\n",
       " 9150,\n",
       " 7709,\n",
       " 4013,\n",
       " 6250,\n",
       " 405,\n",
       " 9317,\n",
       " 4889,\n",
       " 2847,\n",
       " 4626,\n",
       " 7570,\n",
       " 7225,\n",
       " 9576,\n",
       " 4500,\n",
       " 1812,\n",
       " 1068,\n",
       " 202,\n",
       " 1566,\n",
       " 2007,\n",
       " 9317,\n",
       " 9413,\n",
       " 4873,\n",
       " 1566,\n",
       " 6193,\n",
       " 5993,\n",
       " 4341,\n",
       " 7489,\n",
       " 4341,\n",
       " 6613,\n",
       " 6635,\n",
       " 4435,\n",
       " 4435,\n",
       " 1430,\n",
       " 1567,\n",
       " 4087,\n",
       " 4670,\n",
       " 8718,\n",
       " 949,\n",
       " 565,\n",
       " 6635,\n",
       " 4087,\n",
       " 4670,\n",
       " 8718,\n",
       " 8117,\n",
       " 1632,\n",
       " 132,\n",
       " 6958,\n",
       " 8934,\n",
       " 1874,\n",
       " 4435,\n",
       " 2858,\n",
       " 2008,\n",
       " 405,\n",
       " 9477,\n",
       " 6542,\n",
       " 6123,\n",
       " 6635,\n",
       " 5450,\n",
       " 2540,\n",
       " 2196,\n",
       " 3693,\n",
       " 4435,\n",
       " 1771,\n",
       " 4435,\n",
       " 8128,\n",
       " 4273,\n",
       " 771,\n",
       " 6142,\n",
       " 8726,\n",
       " 1122,\n",
       " 762,\n",
       " 4822,\n",
       " 2364,\n",
       " 5663,\n",
       " 9674,\n",
       " 4489,\n",
       " 4435,\n",
       " 6093,\n",
       " 2427,\n",
       " 8842,\n",
       " 7283,\n",
       " 1815,\n",
       " 7341,\n",
       " 1422,\n",
       " 6024,\n",
       " 5489,\n",
       " 4994,\n",
       " 1153,\n",
       " 2900,\n",
       " 3079,\n",
       " 7258,\n",
       " 4435,\n",
       " 5931,\n",
       " 8117,\n",
       " 6635,\n",
       " 2939,\n",
       " 4823,\n",
       " 1422,\n",
       " 1030,\n",
       " 8571,\n",
       " 7515,\n",
       " 9294,\n",
       " 7710,\n",
       " 4435,\n",
       " 8786,\n",
       " 1702,\n",
       " 565,\n",
       " 6635,\n",
       " 9992,\n",
       " 2724,\n",
       " 2777,\n",
       " 9209,\n",
       " 1891,\n",
       " 2515]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size=10000\n",
    "corpus = data[\"full_text\"]\n",
    "onehot_repr=[one_hot(words,voc_size)for words in corpus] \n",
    "# onehot_repr[1]\n",
    "sent_length=50\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "# print(embedded_docs)\n",
    "embedding_vector_features=50\n",
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
    "model.add(Bidirectional(LSTM(100))) \n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(embedded_docs, data[\"fraudulent\"], test_size=0.3, random_state=0)\n",
    "model.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=32, validation_data=(X_test_lstm, y_test_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "# loss, accuracy = model.evaluate(X_test_lstm, y_test_lstm)\n",
    "\n",
    "# print(f\"Test Accuracy: {accuracy}\")\n",
    "# print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# Get classification report\n",
    "# report = classification_report(y_test_lstm, y_pred.round(),target_names = ['0','1'])\n",
    "# print(\"Classification Report:\")\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM F1 score: 0.7350000000000001\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_lstm)\n",
    "y_pred_binary = (y_pred > 0.5).astype('int32')  \n",
    "f1_test = f1_score(y_test_lstm, y_pred_binary )\n",
    "print(f'LSTM F1 score: {f1_test}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Combined text and numeric</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram and numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.sparse import hstack\n",
    "numeric_features = X_train_full[['has_questions', 'employment_type', 'required_experience', 'required_education', 'missing company information']]\n",
    "combined_features = hstack([\n",
    "    StandardScaler().fit_transform(numeric_features),\n",
    "    X_train_bicv])\n",
    "X_test_numeric_features = X_test_full[['has_questions', 'employment_type', 'required_experience', 'required_education', 'missing company information']]\n",
    "X_test_combined_features = hstack([\n",
    "    StandardScaler().fit_transform(X_test_numeric_features),\n",
    "    X_test_bicv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.8395061728395063\n",
      "SVM F1 Score: 0.745945945945946\n",
      "Random Forest F1 Score: 0.8214285714285714\n",
      "XGBoost F1 Score: 0.7990074441687344\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "    \n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_31 (InputLayer)          [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_16 (Embedding)       (None, 50, 50)       500000      ['input_31[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_16 (Bidirectiona  (None, 200)         120800      ['embedding_16[0][0]']           \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 200)          0           ['bidirectional_16[0][0]']       \n",
      "                                                                                                  \n",
      " input_32 (InputLayer)          [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 204)          0           ['dropout_16[0][0]',             \n",
      "                                                                  'input_32[0][0]']               \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 64)           13120       ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 1)            65          ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 633,985\n",
      "Trainable params: 633,985\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "392/392 [==============================] - 37s 82ms/step - loss: 0.1353 - accuracy: 0.9605 - val_loss: 0.0758 - val_accuracy: 0.9789\n",
      "Epoch 2/10\n",
      "392/392 [==============================] - 26s 65ms/step - loss: 0.0509 - accuracy: 0.9837 - val_loss: 0.0665 - val_accuracy: 0.9791\n",
      "Epoch 3/10\n",
      "392/392 [==============================] - 31s 79ms/step - loss: 0.0195 - accuracy: 0.9944 - val_loss: 0.0872 - val_accuracy: 0.9778\n",
      "Epoch 4/10\n",
      "392/392 [==============================] - 31s 79ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.0978 - val_accuracy: 0.9812\n",
      "Epoch 5/10\n",
      "392/392 [==============================] - 32s 80ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.1165 - val_accuracy: 0.9808\n",
      "Epoch 6/10\n",
      "392/392 [==============================] - 32s 82ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.1275 - val_accuracy: 0.9804\n",
      "Epoch 7/10\n",
      "392/392 [==============================] - 30s 78ms/step - loss: 8.3784e-04 - accuracy: 0.9998 - val_loss: 0.1268 - val_accuracy: 0.9804\n",
      "Epoch 8/10\n",
      "392/392 [==============================] - 33s 84ms/step - loss: 6.4680e-04 - accuracy: 0.9998 - val_loss: 0.1460 - val_accuracy: 0.9793\n",
      "Epoch 9/10\n",
      "392/392 [==============================] - 31s 78ms/step - loss: 4.5933e-04 - accuracy: 0.9998 - val_loss: 0.1542 - val_accuracy: 0.9808\n",
      "Epoch 10/10\n",
      "392/392 [==============================] - 31s 79ms/step - loss: 1.2908e-04 - accuracy: 1.0000 - val_loss: 0.1439 - val_accuracy: 0.9799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffdab129690>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "numerical_data = data[['has_questions', 'employment_type', 'required_experience', 'required_education', 'missing company information']].values\n",
    "\n",
    "# Define text input\n",
    "text_input = Input(shape=(sent_length,))\n",
    "embedding_vector_features = 50\n",
    "text_embedding = Embedding(voc_size, embedding_vector_features, input_length=sent_length)(text_input)\n",
    "text_lstm = Bidirectional(LSTM(100))(text_embedding)\n",
    "text_dropout = Dropout(0.3)(text_lstm)\n",
    "\n",
    "# Define numerical input\n",
    "numerical_input = Input(shape=(numerical_data.shape[1],))\n",
    "\n",
    "# Concatenate text and numerical inputs\n",
    "concatenated = Concatenate()([text_dropout, numerical_input])\n",
    "\n",
    "# Dense layers for the merged inputs\n",
    "dense_layer = Dense(64, activation='relu')(concatenated)\n",
    "output_layer = Dense(1, activation='sigmoid')(dense_layer)\n",
    "\n",
    "# Create and compile the model\n",
    "model = Model(inputs=[text_input, numerical_input], outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#embedded_docs = np.array(embedded_docs)\n",
    "#numerical_data = np.array(numerical_data)\n",
    "labels = np.array(data[\"fraudulent\"])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "text_train, text_test, num_train, num_test, labels_train, labels_test = train_test_split(\n",
    "    embedded_docs, numerical_data, labels, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "# Train the model using both text and numerical data\n",
    "model.fit([text_train, num_train], labels_train, epochs=10, batch_size=32, validation_data=([text_test, num_test], labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 5s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "test_predictions = model.predict([text_test, num_test])\n",
    "y_pred_binary = (test_predictions > 0.5).astype('int32') \n",
    "f1_test = f1_score(y_test_lstm, y_pred_binary )\n",
    "print(f'LSTM F1 score: {f1_test}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Combined features</h3>\n",
    "\n",
    "|  | Logistic Regression | SVM | Random Forest | XGBoost | LSTM |\n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| Text Only | 0.832 | 0.739 | 0.817 | 0.796 | 0.735 |\n",
    "| Text and Numeric | 0.840 | 0.746 | 0.821 | 0.799 | 0.744 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
