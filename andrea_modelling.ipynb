{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessed_data_with_num.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN with empty strings\n",
    "data.fillna('', inplace=True)\n",
    "\n",
    "# combine all text\n",
    "data['full_text'] = data['title'] + \" \" + data['location']  + \" \" + data['department']  + \" \" + data['company_profile']  + \" \" + data['description']  + \" \" + data['requirements']  + \" \"  + data['benefits'] + data['industry']  + \" \" + data['function']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Categorical Only Models</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Logistic Regression | SVM | Random Forest | XGBoost | Average |\n",
    "|----------|----------|----------|----------|----------|\n",
    "|  |  |  |  |  |\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Only Models</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test_full,y_train , y_test = train_test_split(data.drop('fraudulent', axis=1), data[\"fraudulent\"], test_size=0.3, random_state=0)\n",
    "X_train = X_train_full[['full_text', 'has_questions', 'has_company_logo', 'employment_type', 'required_experience', 'required_education']]\n",
    "X_test = X_test_full[['full_text', 'has_questions', 'has_company_logo', 'employment_type', 'required_experience', 'required_education']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['full_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.552278820375335\n",
      "SVM F1 Score: 0.7129186602870813\n",
      "Random Forest F1 Score: 0.7476635514018691\n",
      "XGBoost F1 Score: 0.7849223946784921\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenized_text = X_train['full_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert words to vectors\n",
    "def get_vector(word_list, model):\n",
    "    valid_words = [word for word in word_list if word in model.wv]\n",
    "    if not valid_words:\n",
    "        # If no valid words, return a vector of zeros or handle as needed\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model.wv[word] for word in valid_words], axis=0)\n",
    "\n",
    "X_train_word2vec = tokenized_text.apply(lambda x: get_vector(x, word2vec_model))\n",
    "X_test_word2vec = X_test['full_text'].apply(lambda x: get_vector(x.split(), word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreayeo/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.3691460055096419\n",
      "SVM F1 Score: 0.4550561797752809\n",
      "Random Forest F1 Score: 0.526027397260274\n",
      "XGBoost F1 Score: 0.615\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train.to_list(), y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test.to_list())\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngrams Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train into fraud and non-fraud\n",
    "X_train_nonfraud = X_train.loc[y_train==0]\n",
    "X_train_fraud = X_train.loc[y_train==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to return ngrams sorted by frequency\n",
    "def get_ngrams(ngram, corpus):\n",
    "    vec = CountVectorizer(ngram_range=(ngram, ngram)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = {}\n",
    "    for word, idx in vec.vocabulary_.items():\n",
    "        words_freq[word] = sum_words[0, idx]\n",
    "    words_freq = dict(sorted(words_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('awesome', 1362), ('abroad', 1289), ('athens', 921), ('european', 865), ('berlin', 785)]\n",
      "[('aker', 152), ('accion', 58), ('0fa3f7c5e23a16de16a841e368006cae916884407d90b154dfef3976483a71ae', 53), ('anyperk', 40), ('novation', 40)]\n",
      "[('engineering', 0.16199194664859687, 0.4842268371145133, 0.32223489046591647), ('team', 0.8978269988982973, 0.5835986054267092, 0.31422839347158815), ('position', 0.26084672032725065, 0.5297020531217894, 0.26885533279453877), ('marketing', 0.32690296259294693, 0.10189816920148889, 0.22500479339145804), ('work', 0.884937176053005, 1.0998265204722686, 0.21488934441926355)]\n"
     ]
    }
   ],
   "source": [
    "nonfraud_unigram = get_ngrams(1, X_train_nonfraud['full_text'])\n",
    "fraud_unigram = get_ngrams(1, X_train_fraud['full_text'])\n",
    "nonfraud_unigram_top5 = [(k,v) for k,v in nonfraud_unigram.items() if k not in fraud_unigram.keys()][:5]\n",
    "fraud_unigram_top5 = [(k,v) for k,v in fraud_unigram.items() if k not in nonfraud_unigram.keys()][:5]\n",
    "\n",
    "nonfraud_num_unigram, fraud_num_unigram = sum(nonfraud_unigram.values()), sum(fraud_unigram.values())\n",
    "diff_unigram = [(k, nonfraud_unigram[k]*100/nonfraud_num_unigram, fraud_unigram[k]*100/fraud_num_unigram, \n",
    "                 abs((nonfraud_unigram[k]*100/nonfraud_num_unigram)-(fraud_unigram[k]*100/fraud_num_unigram))) for k in nonfraud_unigram.keys() if k in fraud_unigram.keys()]\n",
    "diff_unigram = sorted(diff_unigram, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "print(nonfraud_unigram_top5)\n",
    "print(fraud_unigram_top5)\n",
    "print(diff_unigram[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('university degree', 820), ('increase productivity', 783), ('document communication', 773), ('relevant job', 709), ('digital marketing', 617)]\n",
      "[('aker solution', 148), ('aptitude staffing', 76), ('bring discovery', 53), ('production maximize', 53), ('maximize recovery', 53)]\n",
      "[('data entry', 0.008330150535368569, 0.1887335387115339, 0.18040338817616533), ('oil gas', 0.011754402138840232, 0.12610447205389486, 0.11435006991505463), ('customer service', 0.15939232704236853, 0.2657503639256576, 0.10635803688328907), ('work home', 0.006914354199317784, 0.09648261620230882, 0.08956826200299103), ('gas industry', 0.005498557863267, 0.08717289007752463, 0.08167433221425763)]\n"
     ]
    }
   ],
   "source": [
    "nonfraud_bigram = get_ngrams(2, X_train_nonfraud['full_text'])\n",
    "fraud_bigram = get_ngrams(2,  X_train_fraud['full_text'])\n",
    "nonfraud_bigram_top5 = [(k,v) for k,v in nonfraud_bigram.items() if k not in fraud_bigram.keys()][:5]\n",
    "fraud_bigram_top5 = [(k,v) for k,v in fraud_bigram.items() if k not in nonfraud_bigram.keys()][:5]\n",
    "\n",
    "nonfraud_num_bigram, fraud_num_bigram = sum(nonfraud_bigram.values()), sum(fraud_bigram.values())\n",
    "diff_bigram = [(k, nonfraud_bigram[k]*100/nonfraud_num_bigram, fraud_bigram[k]*100/fraud_num_bigram, \n",
    "                abs((nonfraud_bigram[k]*100/nonfraud_num_bigram)-(fraud_bigram[k]*100/fraud_num_bigram))) for k in nonfraud_bigram.keys() if k in fraud_bigram.keys()]\n",
    "diff_bigram = sorted(diff_bigram, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "print(nonfraud_bigram_top5)\n",
    "print(fraud_bigram_top5)\n",
    "print(diff_bigram[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('full time permanent', 587), ('time permanent position', 563), ('permanent position many', 550), ('position many medium', 550), ('many medium large', 550)]\n",
      "[('gas industry engineering', 55), ('28 000 people', 55), ('aker solution global', 53), ('solution global provider', 53), ('global provider product', 53)]\n",
      "[('oil gas industry', 0.00551992577517772, 0.0876103635404794, 0.08209043776530167), ('usa tx houston', 0.0038341999396444043, 0.05103516322746372, 0.047200963287819316), ('product system service', 3.305344775555521e-05, 0.04508106085092629, 0.045048007403170734), ('approximately 28 000', 6.610689551111042e-05, 0.04508106085092629, 0.045014953955415174), ('service oil gas', 0.0003635879253111073, 0.04508106085092629, 0.04471747292561518)]\n"
     ]
    }
   ],
   "source": [
    "nonfraud_trigram = get_ngrams(3,  X_train_nonfraud['full_text'])\n",
    "fraud_trigram = get_ngrams(3,  X_train_fraud['full_text'])\n",
    "nonfraud_trigram_top5 = [(k,v) for k,v in nonfraud_trigram.items() if k not in fraud_trigram.keys()][:5]\n",
    "fraud_trigram_top5 = [(k,v) for k,v in fraud_trigram.items() if k not in nonfraud_trigram.keys()][:5]\n",
    "\n",
    "nonfraud_num_trigram, fraud_num_trigram = sum(nonfraud_trigram.values()), sum(fraud_trigram.values())\n",
    "diff_trigram = [(k, nonfraud_trigram[k]*100/nonfraud_num_trigram, fraud_trigram[k]*100/fraud_num_trigram, \n",
    "                 abs((nonfraud_trigram[k]*100/nonfraud_num_trigram)-(fraud_trigram[k]*100/fraud_num_trigram))) for k in nonfraud_trigram.keys() if k in fraud_trigram.keys()]\n",
    "diff_trigram = sorted(diff_trigram, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "print(nonfraud_trigram_top5)\n",
    "print(fraud_trigram_top5)\n",
    "print(diff_trigram[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer - Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X_train_cv = count_vectorizer.fit_transform(X_train['full_text'])\n",
    "X_test_cv = count_vectorizer.transform(X_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreayeo/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.7975206611570248\n",
      "SVM F1 Score: 0.6157760814249365\n",
      "Random Forest F1 Score: 0.7323943661971831\n",
      "XGBoost F1 Score: 0.810344827586207\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X_train_bicv = count_vectorizer.fit_transform(X_train['full_text'])\n",
    "X_test_bicv = count_vectorizer.transform(X_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.7937915742793792\n",
      "SVM F1 Score: 0.671604938271605\n",
      "Random Forest F1 Score: 0.7775280898876403\n",
      "XGBoost F1 Score: 0.8008752735229759\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using text features only</h3>\n",
    "\n",
    "|  | Logistic Regression | SVM | Random Forest | XGBoost | Average |\n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| TFIDF | 0.552 | 0.713 | 0.748 | 0.785 | 0.700 |\n",
    "| Word2Vec | 0.369 | 0.455 | 0.526 | 0.615 | 0.491 |\n",
    "| CountVectorizer - Unigram | 0.798 | 0.616 | 0.732 | 0.810 | 0.739 |\n",
    "| CountVectorizer - Bigram | 0.794 | 0.672 | 0.778 | 0.801 | 0.761 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, the best word embedding method to use is CountVectorizer - Bigram that obtained the highest average F1 score of 0.761 across all models. Hence, we will be using CountVectorizer - Bigram moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 11:26:54.813638: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense,Bidirectional\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "# from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1891,\n",
       " 2515,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 9489,\n",
       " 5111,\n",
       " 1215,\n",
       " 6958,\n",
       " 8934,\n",
       " 7709,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 2515,\n",
       " 6958,\n",
       " 8934,\n",
       " 7709,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 2515,\n",
       " 3866,\n",
       " 6250,\n",
       " 3559,\n",
       " 8786,\n",
       " 4364,\n",
       " 5708,\n",
       " 5569,\n",
       " 4435,\n",
       " 4993,\n",
       " 4672,\n",
       " 4437,\n",
       " 8096,\n",
       " 7709,\n",
       " 6958,\n",
       " 8934,\n",
       " 2152,\n",
       " 4435,\n",
       " 1068,\n",
       " 9063,\n",
       " 4103,\n",
       " 3359,\n",
       " 7822,\n",
       " 2552,\n",
       " 4267,\n",
       " 4360,\n",
       " 7102,\n",
       " 6958,\n",
       " 9609,\n",
       " 3947,\n",
       " 6958,\n",
       " 8934,\n",
       " 9261,\n",
       " 9255,\n",
       " 1872,\n",
       " 2307,\n",
       " 9531,\n",
       " 4751,\n",
       " 9317,\n",
       " 7664,\n",
       " 4435,\n",
       " 1068,\n",
       " 6244,\n",
       " 9318,\n",
       " 8556,\n",
       " 5286,\n",
       " 4435,\n",
       " 7225,\n",
       " 9837,\n",
       " 5569,\n",
       " 405,\n",
       " 5512,\n",
       " 74,\n",
       " 7864,\n",
       " 4512,\n",
       " 4341,\n",
       " 7098,\n",
       " 4435,\n",
       " 7436,\n",
       " 1693,\n",
       " 6288,\n",
       " 3359,\n",
       " 64,\n",
       " 1068,\n",
       " 1215,\n",
       " 1566,\n",
       " 9620,\n",
       " 6288,\n",
       " 6958,\n",
       " 8934,\n",
       " 8918,\n",
       " 6169,\n",
       " 1215,\n",
       " 3061,\n",
       " 6958,\n",
       " 8934,\n",
       " 4437,\n",
       " 3326,\n",
       " 6389,\n",
       " 4341,\n",
       " 4435,\n",
       " 8978,\n",
       " 6288,\n",
       " 4690,\n",
       " 74,\n",
       " 6250,\n",
       " 3005,\n",
       " 7709,\n",
       " 4175,\n",
       " 3005,\n",
       " 5615,\n",
       " 2579,\n",
       " 9802,\n",
       " 6418,\n",
       " 1200,\n",
       " 3225,\n",
       " 5111,\n",
       " 9147,\n",
       " 1784,\n",
       " 4529,\n",
       " 9872,\n",
       " 7102,\n",
       " 6958,\n",
       " 9609,\n",
       " 3947,\n",
       " 7102,\n",
       " 6958,\n",
       " 9609,\n",
       " 5708,\n",
       " 7102,\n",
       " 6958,\n",
       " 9609,\n",
       " 6073,\n",
       " 185,\n",
       " 9592,\n",
       " 3497,\n",
       " 4318,\n",
       " 4129,\n",
       " 1891,\n",
       " 2515,\n",
       " 1093,\n",
       " 3805,\n",
       " 9101,\n",
       " 382,\n",
       " 5070,\n",
       " 161,\n",
       " 9477,\n",
       " 5209,\n",
       " 2799,\n",
       " 8657,\n",
       " 5516,\n",
       " 62,\n",
       " 870,\n",
       " 9625,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 2515,\n",
       " 6198,\n",
       " 5591,\n",
       " 8731,\n",
       " 4600,\n",
       " 5684,\n",
       " 6189,\n",
       " 97,\n",
       " 2710,\n",
       " 7709,\n",
       " 925,\n",
       " 8899,\n",
       " 4412,\n",
       " 1891,\n",
       " 2515,\n",
       " 5047,\n",
       " 2832,\n",
       " 1430,\n",
       " 8920,\n",
       " 8772,\n",
       " 7225,\n",
       " 2185,\n",
       " 5548,\n",
       " 8560,\n",
       " 4474,\n",
       " 4435,\n",
       " 7225,\n",
       " 5548,\n",
       " 2847,\n",
       " 2266,\n",
       " 4500,\n",
       " 1068,\n",
       " 2067,\n",
       " 2142,\n",
       " 8552,\n",
       " 6250,\n",
       " 6240,\n",
       " 338,\n",
       " 74,\n",
       " 1566,\n",
       " 5630,\n",
       " 9489,\n",
       " 7312,\n",
       " 8718,\n",
       " 6696,\n",
       " 9515,\n",
       " 3872,\n",
       " 9037,\n",
       " 1430,\n",
       " 5512,\n",
       " 6999,\n",
       " 7374,\n",
       " 8263,\n",
       " 4282,\n",
       " 3049,\n",
       " 9576,\n",
       " 9206,\n",
       " 6131,\n",
       " 6696,\n",
       " 1430,\n",
       " 9206,\n",
       " 6175,\n",
       " 6958,\n",
       " 8934,\n",
       " 7709,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 2515,\n",
       " 7102,\n",
       " 6958,\n",
       " 9609,\n",
       " 3947,\n",
       " 6958,\n",
       " 8934,\n",
       " 7709,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 2515,\n",
       " 3866,\n",
       " 6250,\n",
       " 3559,\n",
       " 8786,\n",
       " 4364,\n",
       " 5708,\n",
       " 5569,\n",
       " 4435,\n",
       " 4993,\n",
       " 4672,\n",
       " 4437,\n",
       " 8096,\n",
       " 7709,\n",
       " 9063,\n",
       " 4103,\n",
       " 3359,\n",
       " 7822,\n",
       " 2552,\n",
       " 4267,\n",
       " 4360,\n",
       " 6958,\n",
       " 8934,\n",
       " 9261,\n",
       " 9255,\n",
       " 1872,\n",
       " 2307,\n",
       " 9531,\n",
       " 4751,\n",
       " 9317,\n",
       " 7664,\n",
       " 4435,\n",
       " 1068,\n",
       " 6244,\n",
       " 9318,\n",
       " 8556,\n",
       " 5286,\n",
       " 4435,\n",
       " 7225,\n",
       " 9837,\n",
       " 5569,\n",
       " 405,\n",
       " 5512,\n",
       " 7864,\n",
       " 4512,\n",
       " 4341,\n",
       " 7098,\n",
       " 4435,\n",
       " 7436,\n",
       " 1693,\n",
       " 6288,\n",
       " 64,\n",
       " 1068,\n",
       " 1215,\n",
       " 1566,\n",
       " 9620,\n",
       " 6288,\n",
       " 5277,\n",
       " 4435,\n",
       " 7225,\n",
       " 1215,\n",
       " 6169,\n",
       " 7041,\n",
       " 911,\n",
       " 8802,\n",
       " 5941,\n",
       " 499,\n",
       " 3960,\n",
       " 6958,\n",
       " 8934,\n",
       " 4437,\n",
       " 3326,\n",
       " 6389,\n",
       " 4341,\n",
       " 4435,\n",
       " 8978,\n",
       " 6288,\n",
       " 4690,\n",
       " 74,\n",
       " 6250,\n",
       " 3005,\n",
       " 7709,\n",
       " 4175,\n",
       " 3005,\n",
       " 5615,\n",
       " 2579,\n",
       " 9802,\n",
       " 6418,\n",
       " 1200,\n",
       " 3225,\n",
       " 5111,\n",
       " 9147,\n",
       " 1784,\n",
       " 4529,\n",
       " 9872,\n",
       " 5111,\n",
       " 3225,\n",
       " 5663,\n",
       " 9206,\n",
       " 3231,\n",
       " 8306,\n",
       " 6723,\n",
       " 8540,\n",
       " 3714,\n",
       " 9450,\n",
       " 7864,\n",
       " 6496,\n",
       " 5520,\n",
       " 1868,\n",
       " 2847,\n",
       " 6958,\n",
       " 8934,\n",
       " 1566,\n",
       " 9064,\n",
       " 7056,\n",
       " 1081,\n",
       " 4435,\n",
       " 1068,\n",
       " 2067,\n",
       " 3005,\n",
       " 2703,\n",
       " 3039,\n",
       " 8778,\n",
       " 9064,\n",
       " 2111,\n",
       " 9318,\n",
       " 7950,\n",
       " 5569,\n",
       " 1068,\n",
       " 161,\n",
       " 8007,\n",
       " 9563,\n",
       " 5548,\n",
       " 4435,\n",
       " 7225,\n",
       " 6856,\n",
       " 1875,\n",
       " 4873,\n",
       " 4435,\n",
       " 2847,\n",
       " 4884,\n",
       " 6496,\n",
       " 4419,\n",
       " 2847,\n",
       " 9592,\n",
       " 4412,\n",
       " 1891,\n",
       " 2515,\n",
       " 2266,\n",
       " 9101,\n",
       " 5569,\n",
       " 3031,\n",
       " 4104,\n",
       " 4574,\n",
       " 405,\n",
       " 4847,\n",
       " 5569,\n",
       " 2792,\n",
       " 7225,\n",
       " 161,\n",
       " 2129,\n",
       " 185,\n",
       " 5548,\n",
       " 5859,\n",
       " 5376,\n",
       " 5440,\n",
       " 7033,\n",
       " 2487,\n",
       " 5091,\n",
       " 8146,\n",
       " 3647,\n",
       " 6205,\n",
       " 4129,\n",
       " 4412,\n",
       " 2142,\n",
       " 6664,\n",
       " 4873,\n",
       " 5484,\n",
       " 1344,\n",
       " 6667,\n",
       " 9487,\n",
       " 9477,\n",
       " 4191,\n",
       " 6353,\n",
       " 1179,\n",
       " 4634,\n",
       " 538,\n",
       " 9063,\n",
       " 647,\n",
       " 2142,\n",
       " 3170,\n",
       " 419,\n",
       " 7041,\n",
       " 384,\n",
       " 5418,\n",
       " 9665,\n",
       " 8786,\n",
       " 5346,\n",
       " 4884,\n",
       " 9477,\n",
       " 238,\n",
       " 8415,\n",
       " 1554,\n",
       " 8252,\n",
       " 9317,\n",
       " 7151,\n",
       " 3883,\n",
       " 8359,\n",
       " 1658,\n",
       " 2412,\n",
       " 3005,\n",
       " 3226,\n",
       " 7225,\n",
       " 6740,\n",
       " 2487,\n",
       " 7948,\n",
       " 405,\n",
       " 6958,\n",
       " 8934,\n",
       " 1057,\n",
       " 1723,\n",
       " 405,\n",
       " 1891,\n",
       " 2515,\n",
       " 7225,\n",
       " 161,\n",
       " 4870,\n",
       " 7989,\n",
       " 8110,\n",
       " 6445,\n",
       " 9609,\n",
       " 7559,\n",
       " 8786,\n",
       " 2896,\n",
       " 1567,\n",
       " 8502,\n",
       " 8786,\n",
       " 2008,\n",
       " 2598,\n",
       " 6958,\n",
       " 8934,\n",
       " 1566,\n",
       " 8859,\n",
       " 405,\n",
       " 9317,\n",
       " 7225,\n",
       " 2664,\n",
       " 9150,\n",
       " 7709,\n",
       " 4013,\n",
       " 6250,\n",
       " 405,\n",
       " 9317,\n",
       " 4889,\n",
       " 2847,\n",
       " 4626,\n",
       " 7570,\n",
       " 7225,\n",
       " 9576,\n",
       " 4500,\n",
       " 1812,\n",
       " 1068,\n",
       " 202,\n",
       " 1566,\n",
       " 2007,\n",
       " 9317,\n",
       " 9413,\n",
       " 4873,\n",
       " 1566,\n",
       " 6193,\n",
       " 5993,\n",
       " 4341,\n",
       " 7489,\n",
       " 4341,\n",
       " 6613,\n",
       " 6635,\n",
       " 4435,\n",
       " 4435,\n",
       " 1430,\n",
       " 1567,\n",
       " 4087,\n",
       " 4670,\n",
       " 8718,\n",
       " 949,\n",
       " 565,\n",
       " 6635,\n",
       " 4087,\n",
       " 4670,\n",
       " 8718,\n",
       " 8117,\n",
       " 1632,\n",
       " 132,\n",
       " 6958,\n",
       " 8934,\n",
       " 1874,\n",
       " 4435,\n",
       " 2858,\n",
       " 2008,\n",
       " 405,\n",
       " 9477,\n",
       " 6542,\n",
       " 6123,\n",
       " 6635,\n",
       " 5450,\n",
       " 2540,\n",
       " 2196,\n",
       " 3693,\n",
       " 4435,\n",
       " 1771,\n",
       " 4435,\n",
       " 8128,\n",
       " 4273,\n",
       " 771,\n",
       " 6142,\n",
       " 8726,\n",
       " 1122,\n",
       " 762,\n",
       " 4822,\n",
       " 2364,\n",
       " 5663,\n",
       " 9674,\n",
       " 4489,\n",
       " 4435,\n",
       " 6093,\n",
       " 2427,\n",
       " 8842,\n",
       " 7283,\n",
       " 1815,\n",
       " 7341,\n",
       " 1422,\n",
       " 6024,\n",
       " 5489,\n",
       " 4994,\n",
       " 1153,\n",
       " 2900,\n",
       " 3079,\n",
       " 7258,\n",
       " 4435,\n",
       " 5931,\n",
       " 8117,\n",
       " 6635,\n",
       " 2939,\n",
       " 4823,\n",
       " 1422,\n",
       " 1030,\n",
       " 8571,\n",
       " 7515,\n",
       " 9294,\n",
       " 7710,\n",
       " 4435,\n",
       " 8786,\n",
       " 1702,\n",
       " 565,\n",
       " 6635,\n",
       " 9992,\n",
       " 2724,\n",
       " 2777,\n",
       " 9209,\n",
       " 1891,\n",
       " 2515]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size=10000\n",
    "corpus = data[\"full_text\"]\n",
    "onehot_repr=[one_hot(words,voc_size)for words in corpus] \n",
    "# onehot_repr[1]\n",
    "sent_length=50\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "# print(embedded_docs)\n",
    "embedding_vector_features=50\n",
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
    "model.add(Bidirectional(LSTM(100))) \n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(embedded_docs, data[\"fraudulent\"], test_size=0.3, random_state=0)\n",
    "model.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=32, validation_data=(X_test_lstm, y_test_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "# loss, accuracy = model.evaluate(X_test_lstm, y_test_lstm)\n",
    "\n",
    "# print(f\"Test Accuracy: {accuracy}\")\n",
    "# print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# Get classification report\n",
    "# report = classification_report(y_test_lstm, y_pred.round(),target_names = ['0','1'])\n",
    "# print(\"Classification Report:\")\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM F1 score: 0.7350000000000001\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_lstm)\n",
    "y_pred_binary = (y_pred > 0.5).astype('int32')  \n",
    "f1_test = f1_score(y_test_lstm, y_pred_binary )\n",
    "print(f'LSTM F1 score: {f1_test}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Combined text and numeric</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram and numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X_train_full[['has_questions', 'has_company_logo', 'employment_type', 'required_experience', 'required_education']]\n",
    "combined_features = hstack([\n",
    "    StandardScaler().fit_transform(numeric_features),\n",
    "    X_train_bicv])\n",
    "X_test_numeric_features = X_test_full[['has_questions', 'has_company_logo', 'employment_type', 'required_experience', 'required_education']]\n",
    "X_test_combined_features = hstack([\n",
    "    StandardScaler().fit_transform(X_test_numeric_features),\n",
    "    X_test_bicv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.8026315789473685\n",
      "SVM F1 Score: 0.6941747572815535\n",
      "Random Forest F1 Score: 0.7681818181818182\n",
      "XGBoost F1 Score: 0.8197424892703862\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "    \n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_31 (InputLayer)          [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_16 (Embedding)       (None, 50, 50)       500000      ['input_31[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_16 (Bidirectiona  (None, 200)         120800      ['embedding_16[0][0]']           \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 200)          0           ['bidirectional_16[0][0]']       \n",
      "                                                                                                  \n",
      " input_32 (InputLayer)          [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 204)          0           ['dropout_16[0][0]',             \n",
      "                                                                  'input_32[0][0]']               \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 64)           13120       ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 1)            65          ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 633,985\n",
      "Trainable params: 633,985\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "392/392 [==============================] - 37s 82ms/step - loss: 0.1353 - accuracy: 0.9605 - val_loss: 0.0758 - val_accuracy: 0.9789\n",
      "Epoch 2/10\n",
      "392/392 [==============================] - 26s 65ms/step - loss: 0.0509 - accuracy: 0.9837 - val_loss: 0.0665 - val_accuracy: 0.9791\n",
      "Epoch 3/10\n",
      "392/392 [==============================] - 31s 79ms/step - loss: 0.0195 - accuracy: 0.9944 - val_loss: 0.0872 - val_accuracy: 0.9778\n",
      "Epoch 4/10\n",
      "392/392 [==============================] - 31s 79ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.0978 - val_accuracy: 0.9812\n",
      "Epoch 5/10\n",
      "392/392 [==============================] - 32s 80ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.1165 - val_accuracy: 0.9808\n",
      "Epoch 6/10\n",
      "392/392 [==============================] - 32s 82ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.1275 - val_accuracy: 0.9804\n",
      "Epoch 7/10\n",
      "392/392 [==============================] - 30s 78ms/step - loss: 8.3784e-04 - accuracy: 0.9998 - val_loss: 0.1268 - val_accuracy: 0.9804\n",
      "Epoch 8/10\n",
      "392/392 [==============================] - 33s 84ms/step - loss: 6.4680e-04 - accuracy: 0.9998 - val_loss: 0.1460 - val_accuracy: 0.9793\n",
      "Epoch 9/10\n",
      "392/392 [==============================] - 31s 78ms/step - loss: 4.5933e-04 - accuracy: 0.9998 - val_loss: 0.1542 - val_accuracy: 0.9808\n",
      "Epoch 10/10\n",
      "392/392 [==============================] - 31s 79ms/step - loss: 1.2908e-04 - accuracy: 1.0000 - val_loss: 0.1439 - val_accuracy: 0.9799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffdab129690>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from tensorflow.keras.layers import Input, Concatenate\n",
    "# from tensorflow.keras.models import Model\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "numerical_data = data[['has_questions', 'has_company_logo', 'employment_type', 'required_experience', 'required_education']].values\n",
    "\n",
    "# Define text input\n",
    "text_input = Input(shape=(sent_length,))\n",
    "embedding_vector_features = 50\n",
    "text_embedding = Embedding(voc_size, embedding_vector_features, input_length=sent_length)(text_input)\n",
    "text_lstm = Bidirectional(LSTM(100))(text_embedding)\n",
    "text_dropout = Dropout(0.3)(text_lstm)\n",
    "\n",
    "# Define numerical input\n",
    "numerical_input = Input(shape=(numerical_data.shape[1],))\n",
    "\n",
    "# Concatenate text and numerical inputs\n",
    "concatenated = Concatenate()([text_dropout, numerical_input])\n",
    "\n",
    "# Dense layers for the merged inputs\n",
    "dense_layer = Dense(64, activation='relu')(concatenated)\n",
    "output_layer = Dense(1, activation='sigmoid')(dense_layer)\n",
    "\n",
    "# Create and compile the model\n",
    "model = Model(inputs=[text_input, numerical_input], outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#embedded_docs = np.array(embedded_docs)\n",
    "#numerical_data = np.array(numerical_data)\n",
    "labels = np.array(data[\"fraudulent\"])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "text_train, text_test, num_train, num_test, labels_train, labels_test = train_test_split(\n",
    "    embedded_docs, numerical_data, labels, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "# Train the model using both text and numerical data\n",
    "model.fit([text_train, num_train], labels_train, epochs=10, batch_size=32, validation_data=([text_test, num_test], labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 5s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "test_predictions = model.predict([text_test, num_test])\n",
    "y_pred_binary = (test_predictions > 0.5).astype('int32') \n",
    "f1_test = f1_score(y_test_lstm, y_pred_binary )\n",
    "print(f'LSTM F1 score: {f1_test}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Combined features</h3>\n",
    "\n",
    "|  | Logistic Regression | SVM | Random Forest | XGBoost | LSTM |\n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| Text Only | 0.794 | 0.672 | 0.778 | 0.801 | 0.735 |\n",
    "| Text and Numeric | 0.803 | 0.694 | 0.768 | 0.820 | 0.744 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
