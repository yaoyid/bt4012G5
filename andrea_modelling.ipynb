{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('preprocessed_data_with_numerical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['full_text'] = data['title'] + \" \" + data['location']  + \" \" + data['department']  + \" \" + data['company_profile']  + \" \" + data['description']  + \" \" + data['requirements']  + \" \"  + data['benefits'] + data['industry']  + \" \" + data['function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>company_profile</th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "      <th>benefits</th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>required_experience</th>\n",
       "      <th>required_education</th>\n",
       "      <th>industry</th>\n",
       "      <th>function</th>\n",
       "      <th>fraudulent</th>\n",
       "      <th>in_balanced_dataset</th>\n",
       "      <th>missing company profile</th>\n",
       "      <th>missing company information</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marketing intern</td>\n",
       "      <td>usa ny new york</td>\n",
       "      <td>marketing</td>\n",
       "      <td>food52 created groundbreaking award winning co...</td>\n",
       "      <td>food52 fast growing james beard award winning ...</td>\n",
       "      <td>experience content management system major plu...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td>marketing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>marketing intern usa ny new york marketing foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>customer service cloud video production</td>\n",
       "      <td>nz auckland</td>\n",
       "      <td>success</td>\n",
       "      <td>90 second world cloud video production service...</td>\n",
       "      <td>organised focused vibrant awesome passion cust...</td>\n",
       "      <td>expect key responsibility communicate client 9...</td>\n",
       "      <td>get u part 90 second team gain experience work...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>marketing advertising</td>\n",
       "      <td>customer service</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>customer service cloud video production nz auc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>commissioning machinery assistant cma</td>\n",
       "      <td>usa ia wever</td>\n",
       "      <td></td>\n",
       "      <td>valor service provides workforce solution meet...</td>\n",
       "      <td>client located houston actively seeking experi...</td>\n",
       "      <td>implement pre commissioning commissioning proc...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>commissioning machinery assistant cma usa ia w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>account executive washington dc</td>\n",
       "      <td>usa dc washington</td>\n",
       "      <td>sale</td>\n",
       "      <td>passion improving quality life geography heart...</td>\n",
       "      <td>company esri environmental system research ins...</td>\n",
       "      <td>education bachelor master gi business administ...</td>\n",
       "      <td>culture anything corporate collaborative creat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>computer software</td>\n",
       "      <td>sale</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>account executive washington dc usa dc washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bill review manager</td>\n",
       "      <td>usa fl fort worth</td>\n",
       "      <td></td>\n",
       "      <td>spotsource solution llc global human capital m...</td>\n",
       "      <td>job title itemization review manager location ...</td>\n",
       "      <td>qualification rn license state texas diploma b...</td>\n",
       "      <td>full benefit offered</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>hospital health care</td>\n",
       "      <td>health care provider</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>bill review manager usa fl fort worth  spotsou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17875</th>\n",
       "      <td>account director distribution</td>\n",
       "      <td>ca toronto</td>\n",
       "      <td>sale</td>\n",
       "      <td>vend looking awesome new talent come join u wo...</td>\n",
       "      <td>case first time visited website vend award win...</td>\n",
       "      <td>ace role eat comprehensive statement work brea...</td>\n",
       "      <td>expect u open culture openly share result inpu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>computer software</td>\n",
       "      <td>sale</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>account director distribution ca toronto sale ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17876</th>\n",
       "      <td>payroll accountant</td>\n",
       "      <td>usa pa philadelphia</td>\n",
       "      <td>accounting</td>\n",
       "      <td>weblinc e commerce platform service provider f...</td>\n",
       "      <td>payroll accountant focus primarily payroll fun...</td>\n",
       "      <td>b b accounting desire fun love genuine passion...</td>\n",
       "      <td>health wellness medical plan prescription drug...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>internet</td>\n",
       "      <td>accounting auditing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>payroll accountant usa pa philadelphia account...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17877</th>\n",
       "      <td>project cost control staff engineer cost contr...</td>\n",
       "      <td>usa tx houston</td>\n",
       "      <td></td>\n",
       "      <td>provide full time permanent position many medi...</td>\n",
       "      <td>experienced project cost control staff enginee...</td>\n",
       "      <td>least 12 year professional experience ability ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>project cost control staff engineer cost contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17878</th>\n",
       "      <td>graphic designer</td>\n",
       "      <td>ng la lagos</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nemsia studio looking experienced visual graph...</td>\n",
       "      <td>1 must fluent latest version corel adobe cc es...</td>\n",
       "      <td>competitive salary compensation based experien...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>graphic design</td>\n",
       "      <td>design</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>graphic designer ng la lagos   nemsia studio l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17879</th>\n",
       "      <td>web application developer</td>\n",
       "      <td>nz n wellington</td>\n",
       "      <td>engineering</td>\n",
       "      <td>vend looking awesome new talent come join u wo...</td>\n",
       "      <td>vend award winning web based point sale softwa...</td>\n",
       "      <td>want hear depth understanding oo programming t...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>computer software</td>\n",
       "      <td>engineering</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>web application developer nz n wellington engi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17880 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title             location  \\\n",
       "0                                       marketing intern      usa ny new york   \n",
       "1                customer service cloud video production          nz auckland   \n",
       "2                  commissioning machinery assistant cma         usa ia wever   \n",
       "3                        account executive washington dc    usa dc washington   \n",
       "4                                    bill review manager    usa fl fort worth   \n",
       "...                                                  ...                  ...   \n",
       "17875                      account director distribution           ca toronto   \n",
       "17876                                 payroll accountant  usa pa philadelphia   \n",
       "17877  project cost control staff engineer cost contr...       usa tx houston   \n",
       "17878                                   graphic designer          ng la lagos   \n",
       "17879                          web application developer      nz n wellington   \n",
       "\n",
       "        department                                    company_profile  \\\n",
       "0        marketing  food52 created groundbreaking award winning co...   \n",
       "1          success  90 second world cloud video production service...   \n",
       "2                   valor service provides workforce solution meet...   \n",
       "3             sale  passion improving quality life geography heart...   \n",
       "4                   spotsource solution llc global human capital m...   \n",
       "...            ...                                                ...   \n",
       "17875         sale  vend looking awesome new talent come join u wo...   \n",
       "17876   accounting  weblinc e commerce platform service provider f...   \n",
       "17877               provide full time permanent position many medi...   \n",
       "17878                                                                   \n",
       "17879  engineering  vend looking awesome new talent come join u wo...   \n",
       "\n",
       "                                             description  \\\n",
       "0      food52 fast growing james beard award winning ...   \n",
       "1      organised focused vibrant awesome passion cust...   \n",
       "2      client located houston actively seeking experi...   \n",
       "3      company esri environmental system research ins...   \n",
       "4      job title itemization review manager location ...   \n",
       "...                                                  ...   \n",
       "17875  case first time visited website vend award win...   \n",
       "17876  payroll accountant focus primarily payroll fun...   \n",
       "17877  experienced project cost control staff enginee...   \n",
       "17878  nemsia studio looking experienced visual graph...   \n",
       "17879  vend award winning web based point sale softwa...   \n",
       "\n",
       "                                            requirements  \\\n",
       "0      experience content management system major plu...   \n",
       "1      expect key responsibility communicate client 9...   \n",
       "2      implement pre commissioning commissioning proc...   \n",
       "3      education bachelor master gi business administ...   \n",
       "4      qualification rn license state texas diploma b...   \n",
       "...                                                  ...   \n",
       "17875  ace role eat comprehensive statement work brea...   \n",
       "17876  b b accounting desire fun love genuine passion...   \n",
       "17877  least 12 year professional experience ability ...   \n",
       "17878  1 must fluent latest version corel adobe cc es...   \n",
       "17879  want hear depth understanding oo programming t...   \n",
       "\n",
       "                                                benefits  telecommuting  \\\n",
       "0                                                                     0   \n",
       "1      get u part 90 second team gain experience work...              0   \n",
       "2                                                                     0   \n",
       "3      culture anything corporate collaborative creat...              0   \n",
       "4                                   full benefit offered              0   \n",
       "...                                                  ...            ...   \n",
       "17875  expect u open culture openly share result inpu...              0   \n",
       "17876  health wellness medical plan prescription drug...              0   \n",
       "17877                                                                 0   \n",
       "17878  competitive salary compensation based experien...              0   \n",
       "17879                                                                 0   \n",
       "\n",
       "       has_company_logo  has_questions  employment_type  required_experience  \\\n",
       "0                     1              0                2                    4   \n",
       "1                     1              0                1                    6   \n",
       "2                     1              0                2                    6   \n",
       "3                     1              0                1                    5   \n",
       "4                     1              1                1                    5   \n",
       "...                 ...            ...              ...                  ...   \n",
       "17875                 1              1                1                    5   \n",
       "17876                 1              1                1                    5   \n",
       "17877                 0              0                1                    6   \n",
       "17878                 0              1                0                    6   \n",
       "17879                 1              1                1                    5   \n",
       "\n",
       "       required_education               industry              function  \\\n",
       "0                       9                                    marketing   \n",
       "1                       9  marketing advertising      customer service   \n",
       "2                       9                                                \n",
       "3                       1      computer software                  sale   \n",
       "4                       1   hospital health care  health care provider   \n",
       "...                   ...                    ...                   ...   \n",
       "17875                   9      computer software                  sale   \n",
       "17876                   1               internet   accounting auditing   \n",
       "17877                   9                                                \n",
       "17878                   6         graphic design                design   \n",
       "17879                   9      computer software           engineering   \n",
       "\n",
       "       fraudulent  in_balanced_dataset  missing company profile  \\\n",
       "0               0                    0                        1   \n",
       "1               0                    0                        1   \n",
       "2               0                    0                        1   \n",
       "3               0                    0                        1   \n",
       "4               0                    0                        1   \n",
       "...           ...                  ...                      ...   \n",
       "17875           0                    0                        1   \n",
       "17876           0                    0                        1   \n",
       "17877           0                    0                        1   \n",
       "17878           0                    0                        0   \n",
       "17879           0                    0                        1   \n",
       "\n",
       "       missing company information  \\\n",
       "0                                3   \n",
       "1                                3   \n",
       "2                                3   \n",
       "3                                3   \n",
       "4                                3   \n",
       "...                            ...   \n",
       "17875                            3   \n",
       "17876                            3   \n",
       "17877                            2   \n",
       "17878                            0   \n",
       "17879                            3   \n",
       "\n",
       "                                               full_text  \n",
       "0      marketing intern usa ny new york marketing foo...  \n",
       "1      customer service cloud video production nz auc...  \n",
       "2      commissioning machinery assistant cma usa ia w...  \n",
       "3      account executive washington dc usa dc washing...  \n",
       "4      bill review manager usa fl fort worth  spotsou...  \n",
       "...                                                  ...  \n",
       "17875  account director distribution ca toronto sale ...  \n",
       "17876  payroll accountant usa pa philadelphia account...  \n",
       "17877  project cost control staff engineer cost contr...  \n",
       "17878  graphic designer ng la lagos   nemsia studio l...  \n",
       "17879  web application developer nz n wellington engi...  \n",
       "\n",
       "[17880 rows x 20 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_full, X_test_full,y_train , y_test = train_test_split(data.drop('fraudulent', axis=1), data[\"fraudulent\"], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_full[['full_text', 'has_questions', 'required_experience', 'required_education', 'missing company information']]\n",
    "\n",
    "X_test = X_test_full[['full_text', 'has_questions', 'required_experience', 'required_education', 'missing company information']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "# from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Only Models</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['full_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.625\n",
      "SVM F1 Score: 0.8153846153846154\n",
      "Random Forest F1 Score: 0.793733681462141\n",
      "XGBoost F1 Score: 0.8226600985221675\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Step 4: Logistic Regression\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "# Step 5: Support Vector Machine (SVM)\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "# Step 6: Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "# Step 2: Tokenize the text\n",
    "tokenized_text = X_train['full_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Step 3: Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 4: Convert words to vectors\n",
    "def get_vector(word_list, model):\n",
    "    valid_words = [word for word in word_list if word in model.wv]\n",
    "    if not valid_words:\n",
    "        # If no valid words, return a vector of zeros or handle as needed\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model.wv[word] for word in valid_words], axis=0)\n",
    "\n",
    "X_train_word2vec = tokenized_text.apply(lambda x: get_vector(x, word2vec_model))\n",
    "X_test_word2vec = X_test['full_text'].apply(lambda x: get_vector(x.split(), word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natthaphonkanthawang/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.4024024024024024\n",
      "SVM F1 Score: 0.5077399380804953\n",
      "Random Forest F1 Score: 0.6011904761904762\n",
      "XGBoost F1 Score: 0.7225130890052356\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Train models and evaluate F1 score\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train.to_list(), y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test.to_list())\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Step 6: Logistic Regression\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "# Step 7: Support Vector Machine (SVM)\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "# Step 8: Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_word2vec, y_train, X_test_word2vec, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngrams Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train into fraud and non-fraud\n",
    "X_train_nonfraud = X_train.loc[y_train==0]\n",
    "X_train_fraud = X_train.loc[y_train==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to return ngrams sorted by frequency\n",
    "def get_ngrams(ngram, corpus):\n",
    "    vec = CountVectorizer(ngram_range=(ngram, ngram)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = {}\n",
    "    for word, idx in vec.vocabulary_.items():\n",
    "        words_freq[word] = sum_words[0, idx]\n",
    "    words_freq = dict(sorted(words_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('php', 955), ('tidewater', 950), ('athens', 913), ('european', 808), ('1500', 727)]\n",
      "[('aker', 177), ('0fa3f7c5e23a16de16a841e368006cae916884407d90b154dfef3976483a71ae', 60), ('accion', 53), ('novation', 38), ('ddb080358fa5eecf5a67c649cfb4ffc343c484389f1bbaf2a1cb071e3f2b6e7e', 36)]\n",
      "[('team', 0.9025851637746181, 0.5926581845862268, 0.3099269791883913), ('engineering', 0.16009264936303563, 0.45206692454676795, 0.2919742751837323), ('position', 0.2662371577147788, 0.5444998524180144, 0.27826269470323556), ('skill', 0.5543066058087517, 0.7946124807755045, 0.24030587496675282), ('marketing', 0.3309554627764315, 0.10874462102499573, 0.2222108417514358)]\n"
     ]
    }
   ],
   "source": [
    "nonfraud_unigram = get_ngrams(1, X_train_nonfraud['full_text'])\n",
    "fraud_unigram = get_ngrams(1, X_train_fraud['full_text'])\n",
    "nonfraud_unigram_top5 = [(k,v) for k,v in nonfraud_unigram.items() if k not in fraud_unigram.keys()][:5]\n",
    "fraud_unigram_top5 = [(k,v) for k,v in fraud_unigram.items() if k not in nonfraud_unigram.keys()][:5]\n",
    "\n",
    "nonfraud_num_unigram, fraud_num_unigram = sum(nonfraud_unigram.values()), sum(fraud_unigram.values())\n",
    "diff_unigram = [(k, nonfraud_unigram[k]*100/nonfraud_num_unigram, fraud_unigram[k]*100/fraud_num_unigram, \n",
    "                 abs((nonfraud_unigram[k]*100/nonfraud_num_unigram)-(fraud_unigram[k]*100/fraud_num_unigram))) for k in nonfraud_unigram.keys() if k in fraud_unigram.keys()]\n",
    "diff_unigram = sorted(diff_unigram, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "print(nonfraud_unigram_top5)\n",
    "print(fraud_unigram_top5)\n",
    "print(diff_unigram[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('increase productivity', 800), ('university degree', 795), ('document communication', 788), ('degree required', 696), ('medium large', 612)]\n",
      "[('aker solution', 172), ('aptitude staffing', 88), ('bring discovery', 60), ('production maximize', 60), ('maximize recovery', 60)]\n",
      "[('data entry', 0.008304260280918472, 0.1904717297799427, 0.18216746949902424), ('oil gas', 0.012244713198530764, 0.13504757890135283, 0.12280286570282206), ('work home', 0.007457551389530705, 0.09523586488997135, 0.08777831350044064), ('gas industry', 0.005861830786530685, 0.09211337751652966, 0.08625154672999898), ('signing bonus', 9.769717977551142e-05, 0.07806218433604209, 0.07796448715626658)]\n"
     ]
    }
   ],
   "source": [
    "nonfraud_bigram = get_ngrams(2, X_train_nonfraud['full_text'])\n",
    "fraud_bigram = get_ngrams(2,  X_train_fraud['full_text'])\n",
    "nonfraud_bigram_top5 = [(k,v) for k,v in nonfraud_bigram.items() if k not in fraud_bigram.keys()][:5]\n",
    "fraud_bigram_top5 = [(k,v) for k,v in fraud_bigram.items() if k not in nonfraud_bigram.keys()][:5]\n",
    "\n",
    "nonfraud_num_bigram, fraud_num_bigram = sum(nonfraud_bigram.values()), sum(fraud_bigram.values())\n",
    "diff_bigram = [(k, nonfraud_bigram[k]*100/nonfraud_num_bigram, fraud_bigram[k]*100/fraud_num_bigram, \n",
    "                abs((nonfraud_bigram[k]*100/nonfraud_num_bigram)-(fraud_bigram[k]*100/fraud_num_bigram))) for k in nonfraud_bigram.keys() if k in fraud_bigram.keys()]\n",
    "diff_bigram = sorted(diff_bigram, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "print(nonfraud_bigram_top5)\n",
    "print(fraud_bigram_top5)\n",
    "print(diff_bigram[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('full time permanent', 590), ('time permanent position', 566), ('permanent position many', 553), ('position many medium', 553), ('many medium large', 553)]\n",
      "[('gas industry engineering', 62), ('28 000 people', 61), ('aker solution global', 60), ('solution global provider', 60), ('global provider product', 60)]\n",
      "[('oil gas industry', 0.005884591393588934, 0.09257515847611875, 0.08669056708252981), ('product system service', 6.538434881765482e-05, 0.047072114479382414, 0.04700673013056476), ('approximately 28 000', 6.538434881765482e-05, 0.047072114479382414, 0.04700673013056476), ('service oil gas', 0.00019615304645296446, 0.047072114479382414, 0.04687596143292945), ('usa tx houston', 0.004184598324329909, 0.05099479068599762, 0.04681019236166771)]\n"
     ]
    }
   ],
   "source": [
    "nonfraud_trigram = get_ngrams(3,  X_train_nonfraud['full_text'])\n",
    "fraud_trigram = get_ngrams(3,  X_train_fraud['full_text'])\n",
    "nonfraud_trigram_top5 = [(k,v) for k,v in nonfraud_trigram.items() if k not in fraud_trigram.keys()][:5]\n",
    "fraud_trigram_top5 = [(k,v) for k,v in fraud_trigram.items() if k not in nonfraud_trigram.keys()][:5]\n",
    "\n",
    "nonfraud_num_trigram, fraud_num_trigram = sum(nonfraud_trigram.values()), sum(fraud_trigram.values())\n",
    "diff_trigram = [(k, nonfraud_trigram[k]*100/nonfraud_num_trigram, fraud_trigram[k]*100/fraud_num_trigram, \n",
    "                 abs((nonfraud_trigram[k]*100/nonfraud_num_trigram)-(fraud_trigram[k]*100/fraud_num_trigram))) for k in nonfraud_trigram.keys() if k in fraud_trigram.keys()]\n",
    "diff_trigram = sorted(diff_trigram, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "print(nonfraud_trigram_top5)\n",
    "print(fraud_trigram_top5)\n",
    "print(diff_trigram[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X_train_cv = count_vectorizer.fit_transform(X_train['full_text'])\n",
    "X_test_cv = count_vectorizer.transform(X_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natthaphonkanthawang/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.8009153318077803\n",
      "SVM F1 Score: 0.7272727272727272\n",
      "Random Forest F1 Score: 0.793733681462141\n",
      "XGBoost F1 Score: 0.8382352941176471\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Step 4: Logistic Regression\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "# Step 5: Support Vector Machine (SVM)\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "# Step 6: Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X_train_bicv = count_vectorizer.fit_transform(X_train['full_text'])\n",
    "X_test_bicv = count_vectorizer.transform(X_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.8320802005012532\n",
      "SVM F1 Score: 0.7391304347826088\n",
      "Random Forest F1 Score: 0.8062015503875969\n",
      "XGBoost F1 Score: 0.7959183673469389\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Step 4: Logistic Regression\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "# Step 5: Support Vector Machine (SVM)\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "# Step 6: Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, X_train_bicv, y_train, X_test_bicv, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using text features only</h3>\n",
    "\n",
    "|  | Logistic Regression | SVM | Random Forest | XGBoost | Average |\n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| TFIDF | 0.625 | 0.815 | 0.790 | 0.823 | 0.763 |\n",
    "| Word2Vec | 0.385 | 0.497 | 0.568 | 0.686 | 0.534 |\n",
    "| CountVectorizer - Unigram | 0.801 | 0.727 | 0.791 | 0.838 | 0.789 |\n",
    "| CountVectorizer - Bigram | 0.832 | 0.739 | 0.817 | 0.796 | 0.796 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, the best word embedding method to use is CountVectorizer - Bigram that obtained the highest average F1 score of 0.796 across all models. Hence, we will be using CountVectorizer - Bigram moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 11:26:54.813638: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1891,\n",
       " 2515,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 9489,\n",
       " 5111,\n",
       " 1215,\n",
       " 6958,\n",
       " 8934,\n",
       " 7709,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 2515,\n",
       " 6958,\n",
       " 8934,\n",
       " 7709,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 2515,\n",
       " 3866,\n",
       " 6250,\n",
       " 3559,\n",
       " 8786,\n",
       " 4364,\n",
       " 5708,\n",
       " 5569,\n",
       " 4435,\n",
       " 4993,\n",
       " 4672,\n",
       " 4437,\n",
       " 8096,\n",
       " 7709,\n",
       " 6958,\n",
       " 8934,\n",
       " 2152,\n",
       " 4435,\n",
       " 1068,\n",
       " 9063,\n",
       " 4103,\n",
       " 3359,\n",
       " 7822,\n",
       " 2552,\n",
       " 4267,\n",
       " 4360,\n",
       " 7102,\n",
       " 6958,\n",
       " 9609,\n",
       " 3947,\n",
       " 6958,\n",
       " 8934,\n",
       " 9261,\n",
       " 9255,\n",
       " 1872,\n",
       " 2307,\n",
       " 9531,\n",
       " 4751,\n",
       " 9317,\n",
       " 7664,\n",
       " 4435,\n",
       " 1068,\n",
       " 6244,\n",
       " 9318,\n",
       " 8556,\n",
       " 5286,\n",
       " 4435,\n",
       " 7225,\n",
       " 9837,\n",
       " 5569,\n",
       " 405,\n",
       " 5512,\n",
       " 74,\n",
       " 7864,\n",
       " 4512,\n",
       " 4341,\n",
       " 7098,\n",
       " 4435,\n",
       " 7436,\n",
       " 1693,\n",
       " 6288,\n",
       " 3359,\n",
       " 64,\n",
       " 1068,\n",
       " 1215,\n",
       " 1566,\n",
       " 9620,\n",
       " 6288,\n",
       " 6958,\n",
       " 8934,\n",
       " 8918,\n",
       " 6169,\n",
       " 1215,\n",
       " 3061,\n",
       " 6958,\n",
       " 8934,\n",
       " 4437,\n",
       " 3326,\n",
       " 6389,\n",
       " 4341,\n",
       " 4435,\n",
       " 8978,\n",
       " 6288,\n",
       " 4690,\n",
       " 74,\n",
       " 6250,\n",
       " 3005,\n",
       " 7709,\n",
       " 4175,\n",
       " 3005,\n",
       " 5615,\n",
       " 2579,\n",
       " 9802,\n",
       " 6418,\n",
       " 1200,\n",
       " 3225,\n",
       " 5111,\n",
       " 9147,\n",
       " 1784,\n",
       " 4529,\n",
       " 9872,\n",
       " 7102,\n",
       " 6958,\n",
       " 9609,\n",
       " 3947,\n",
       " 7102,\n",
       " 6958,\n",
       " 9609,\n",
       " 5708,\n",
       " 7102,\n",
       " 6958,\n",
       " 9609,\n",
       " 6073,\n",
       " 185,\n",
       " 9592,\n",
       " 3497,\n",
       " 4318,\n",
       " 4129,\n",
       " 1891,\n",
       " 2515,\n",
       " 1093,\n",
       " 3805,\n",
       " 9101,\n",
       " 382,\n",
       " 5070,\n",
       " 161,\n",
       " 9477,\n",
       " 5209,\n",
       " 2799,\n",
       " 8657,\n",
       " 5516,\n",
       " 62,\n",
       " 870,\n",
       " 9625,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 2515,\n",
       " 6198,\n",
       " 5591,\n",
       " 8731,\n",
       " 4600,\n",
       " 5684,\n",
       " 6189,\n",
       " 97,\n",
       " 2710,\n",
       " 7709,\n",
       " 925,\n",
       " 8899,\n",
       " 4412,\n",
       " 1891,\n",
       " 2515,\n",
       " 5047,\n",
       " 2832,\n",
       " 1430,\n",
       " 8920,\n",
       " 8772,\n",
       " 7225,\n",
       " 2185,\n",
       " 5548,\n",
       " 8560,\n",
       " 4474,\n",
       " 4435,\n",
       " 7225,\n",
       " 5548,\n",
       " 2847,\n",
       " 2266,\n",
       " 4500,\n",
       " 1068,\n",
       " 2067,\n",
       " 2142,\n",
       " 8552,\n",
       " 6250,\n",
       " 6240,\n",
       " 338,\n",
       " 74,\n",
       " 1566,\n",
       " 5630,\n",
       " 9489,\n",
       " 7312,\n",
       " 8718,\n",
       " 6696,\n",
       " 9515,\n",
       " 3872,\n",
       " 9037,\n",
       " 1430,\n",
       " 5512,\n",
       " 6999,\n",
       " 7374,\n",
       " 8263,\n",
       " 4282,\n",
       " 3049,\n",
       " 9576,\n",
       " 9206,\n",
       " 6131,\n",
       " 6696,\n",
       " 1430,\n",
       " 9206,\n",
       " 6175,\n",
       " 6958,\n",
       " 8934,\n",
       " 7709,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 2515,\n",
       " 7102,\n",
       " 6958,\n",
       " 9609,\n",
       " 3947,\n",
       " 6958,\n",
       " 8934,\n",
       " 7709,\n",
       " 2552,\n",
       " 4435,\n",
       " 1068,\n",
       " 2515,\n",
       " 3866,\n",
       " 6250,\n",
       " 3559,\n",
       " 8786,\n",
       " 4364,\n",
       " 5708,\n",
       " 5569,\n",
       " 4435,\n",
       " 4993,\n",
       " 4672,\n",
       " 4437,\n",
       " 8096,\n",
       " 7709,\n",
       " 9063,\n",
       " 4103,\n",
       " 3359,\n",
       " 7822,\n",
       " 2552,\n",
       " 4267,\n",
       " 4360,\n",
       " 6958,\n",
       " 8934,\n",
       " 9261,\n",
       " 9255,\n",
       " 1872,\n",
       " 2307,\n",
       " 9531,\n",
       " 4751,\n",
       " 9317,\n",
       " 7664,\n",
       " 4435,\n",
       " 1068,\n",
       " 6244,\n",
       " 9318,\n",
       " 8556,\n",
       " 5286,\n",
       " 4435,\n",
       " 7225,\n",
       " 9837,\n",
       " 5569,\n",
       " 405,\n",
       " 5512,\n",
       " 7864,\n",
       " 4512,\n",
       " 4341,\n",
       " 7098,\n",
       " 4435,\n",
       " 7436,\n",
       " 1693,\n",
       " 6288,\n",
       " 64,\n",
       " 1068,\n",
       " 1215,\n",
       " 1566,\n",
       " 9620,\n",
       " 6288,\n",
       " 5277,\n",
       " 4435,\n",
       " 7225,\n",
       " 1215,\n",
       " 6169,\n",
       " 7041,\n",
       " 911,\n",
       " 8802,\n",
       " 5941,\n",
       " 499,\n",
       " 3960,\n",
       " 6958,\n",
       " 8934,\n",
       " 4437,\n",
       " 3326,\n",
       " 6389,\n",
       " 4341,\n",
       " 4435,\n",
       " 8978,\n",
       " 6288,\n",
       " 4690,\n",
       " 74,\n",
       " 6250,\n",
       " 3005,\n",
       " 7709,\n",
       " 4175,\n",
       " 3005,\n",
       " 5615,\n",
       " 2579,\n",
       " 9802,\n",
       " 6418,\n",
       " 1200,\n",
       " 3225,\n",
       " 5111,\n",
       " 9147,\n",
       " 1784,\n",
       " 4529,\n",
       " 9872,\n",
       " 5111,\n",
       " 3225,\n",
       " 5663,\n",
       " 9206,\n",
       " 3231,\n",
       " 8306,\n",
       " 6723,\n",
       " 8540,\n",
       " 3714,\n",
       " 9450,\n",
       " 7864,\n",
       " 6496,\n",
       " 5520,\n",
       " 1868,\n",
       " 2847,\n",
       " 6958,\n",
       " 8934,\n",
       " 1566,\n",
       " 9064,\n",
       " 7056,\n",
       " 1081,\n",
       " 4435,\n",
       " 1068,\n",
       " 2067,\n",
       " 3005,\n",
       " 2703,\n",
       " 3039,\n",
       " 8778,\n",
       " 9064,\n",
       " 2111,\n",
       " 9318,\n",
       " 7950,\n",
       " 5569,\n",
       " 1068,\n",
       " 161,\n",
       " 8007,\n",
       " 9563,\n",
       " 5548,\n",
       " 4435,\n",
       " 7225,\n",
       " 6856,\n",
       " 1875,\n",
       " 4873,\n",
       " 4435,\n",
       " 2847,\n",
       " 4884,\n",
       " 6496,\n",
       " 4419,\n",
       " 2847,\n",
       " 9592,\n",
       " 4412,\n",
       " 1891,\n",
       " 2515,\n",
       " 2266,\n",
       " 9101,\n",
       " 5569,\n",
       " 3031,\n",
       " 4104,\n",
       " 4574,\n",
       " 405,\n",
       " 4847,\n",
       " 5569,\n",
       " 2792,\n",
       " 7225,\n",
       " 161,\n",
       " 2129,\n",
       " 185,\n",
       " 5548,\n",
       " 5859,\n",
       " 5376,\n",
       " 5440,\n",
       " 7033,\n",
       " 2487,\n",
       " 5091,\n",
       " 8146,\n",
       " 3647,\n",
       " 6205,\n",
       " 4129,\n",
       " 4412,\n",
       " 2142,\n",
       " 6664,\n",
       " 4873,\n",
       " 5484,\n",
       " 1344,\n",
       " 6667,\n",
       " 9487,\n",
       " 9477,\n",
       " 4191,\n",
       " 6353,\n",
       " 1179,\n",
       " 4634,\n",
       " 538,\n",
       " 9063,\n",
       " 647,\n",
       " 2142,\n",
       " 3170,\n",
       " 419,\n",
       " 7041,\n",
       " 384,\n",
       " 5418,\n",
       " 9665,\n",
       " 8786,\n",
       " 5346,\n",
       " 4884,\n",
       " 9477,\n",
       " 238,\n",
       " 8415,\n",
       " 1554,\n",
       " 8252,\n",
       " 9317,\n",
       " 7151,\n",
       " 3883,\n",
       " 8359,\n",
       " 1658,\n",
       " 2412,\n",
       " 3005,\n",
       " 3226,\n",
       " 7225,\n",
       " 6740,\n",
       " 2487,\n",
       " 7948,\n",
       " 405,\n",
       " 6958,\n",
       " 8934,\n",
       " 1057,\n",
       " 1723,\n",
       " 405,\n",
       " 1891,\n",
       " 2515,\n",
       " 7225,\n",
       " 161,\n",
       " 4870,\n",
       " 7989,\n",
       " 8110,\n",
       " 6445,\n",
       " 9609,\n",
       " 7559,\n",
       " 8786,\n",
       " 2896,\n",
       " 1567,\n",
       " 8502,\n",
       " 8786,\n",
       " 2008,\n",
       " 2598,\n",
       " 6958,\n",
       " 8934,\n",
       " 1566,\n",
       " 8859,\n",
       " 405,\n",
       " 9317,\n",
       " 7225,\n",
       " 2664,\n",
       " 9150,\n",
       " 7709,\n",
       " 4013,\n",
       " 6250,\n",
       " 405,\n",
       " 9317,\n",
       " 4889,\n",
       " 2847,\n",
       " 4626,\n",
       " 7570,\n",
       " 7225,\n",
       " 9576,\n",
       " 4500,\n",
       " 1812,\n",
       " 1068,\n",
       " 202,\n",
       " 1566,\n",
       " 2007,\n",
       " 9317,\n",
       " 9413,\n",
       " 4873,\n",
       " 1566,\n",
       " 6193,\n",
       " 5993,\n",
       " 4341,\n",
       " 7489,\n",
       " 4341,\n",
       " 6613,\n",
       " 6635,\n",
       " 4435,\n",
       " 4435,\n",
       " 1430,\n",
       " 1567,\n",
       " 4087,\n",
       " 4670,\n",
       " 8718,\n",
       " 949,\n",
       " 565,\n",
       " 6635,\n",
       " 4087,\n",
       " 4670,\n",
       " 8718,\n",
       " 8117,\n",
       " 1632,\n",
       " 132,\n",
       " 6958,\n",
       " 8934,\n",
       " 1874,\n",
       " 4435,\n",
       " 2858,\n",
       " 2008,\n",
       " 405,\n",
       " 9477,\n",
       " 6542,\n",
       " 6123,\n",
       " 6635,\n",
       " 5450,\n",
       " 2540,\n",
       " 2196,\n",
       " 3693,\n",
       " 4435,\n",
       " 1771,\n",
       " 4435,\n",
       " 8128,\n",
       " 4273,\n",
       " 771,\n",
       " 6142,\n",
       " 8726,\n",
       " 1122,\n",
       " 762,\n",
       " 4822,\n",
       " 2364,\n",
       " 5663,\n",
       " 9674,\n",
       " 4489,\n",
       " 4435,\n",
       " 6093,\n",
       " 2427,\n",
       " 8842,\n",
       " 7283,\n",
       " 1815,\n",
       " 7341,\n",
       " 1422,\n",
       " 6024,\n",
       " 5489,\n",
       " 4994,\n",
       " 1153,\n",
       " 2900,\n",
       " 3079,\n",
       " 7258,\n",
       " 4435,\n",
       " 5931,\n",
       " 8117,\n",
       " 6635,\n",
       " 2939,\n",
       " 4823,\n",
       " 1422,\n",
       " 1030,\n",
       " 8571,\n",
       " 7515,\n",
       " 9294,\n",
       " 7710,\n",
       " 4435,\n",
       " 8786,\n",
       " 1702,\n",
       " 565,\n",
       " 6635,\n",
       " 9992,\n",
       " 2724,\n",
       " 2777,\n",
       " 9209,\n",
       " 1891,\n",
       " 2515]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size=10000\n",
    "corpus = data[\"full_text\"]\n",
    "onehot_repr=[one_hot(words,voc_size)for words in corpus] \n",
    "onehot_repr[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8324 4255 4412 ... 1874 3170 1951]\n",
      " [8128 4273  771 ... 9209 1891 2515]\n",
      " [2939  684 1430 ... 4592 7225 7374]\n",
      " ...\n",
      " [ 255 4741 6175 ... 8185 9609 4842]\n",
      " [3883 4588 3997 ... 2570 4588 4588]\n",
      " [8799 2111 7358 ... 4104 2792 2037]]\n"
     ]
    }
   ],
   "source": [
    "sent_length=50\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 11:27:04.154128: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            500000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 200)              120800    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 621,001\n",
      "Trainable params: 621,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_features=50\n",
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
    "model.add(Bidirectional(LSTM(100))) ##Just add bidirectional!!, except it would just behave as normal LSTM Model\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(embedded_docs, data[\"fraudulent\"], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "392/392 [==============================] - 38s 84ms/step - loss: 0.1561 - accuracy: 0.9591 - val_loss: 0.0898 - val_accuracy: 0.9769\n",
      "Epoch 2/10\n",
      "392/392 [==============================] - 30s 77ms/step - loss: 0.0758 - accuracy: 0.9779 - val_loss: 0.0812 - val_accuracy: 0.9795\n",
      "Epoch 3/10\n",
      "392/392 [==============================] - 26s 67ms/step - loss: 0.0382 - accuracy: 0.9898 - val_loss: 0.0808 - val_accuracy: 0.9793\n",
      "Epoch 4/10\n",
      "392/392 [==============================] - 25s 64ms/step - loss: 0.0191 - accuracy: 0.9942 - val_loss: 0.1045 - val_accuracy: 0.9756\n",
      "Epoch 5/10\n",
      "392/392 [==============================] - 31s 79ms/step - loss: 0.0101 - accuracy: 0.9965 - val_loss: 0.1117 - val_accuracy: 0.9769\n",
      "Epoch 6/10\n",
      "392/392 [==============================] - 30s 76ms/step - loss: 0.0088 - accuracy: 0.9977 - val_loss: 0.1323 - val_accuracy: 0.9769\n",
      "Epoch 7/10\n",
      "392/392 [==============================] - 34s 87ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.1612 - val_accuracy: 0.9795\n",
      "Epoch 8/10\n",
      "392/392 [==============================] - 29s 74ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.1534 - val_accuracy: 0.9789\n",
      "Epoch 9/10\n",
      "392/392 [==============================] - 26s 66ms/step - loss: 0.0035 - accuracy: 0.9987 - val_loss: 0.1244 - val_accuracy: 0.9789\n",
      "Epoch 10/10\n",
      "392/392 [==============================] - 30s 75ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.1236 - val_accuracy: 0.9802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffdea636bc0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=32, validation_data=(X_test_lstm, y_test_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "# loss, accuracy = model.evaluate(X_test_lstm, y_test_lstm)\n",
    "\n",
    "# print(f\"Test Accuracy: {accuracy}\")\n",
    "# print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 5s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "y_pred_binary = (y_pred > 0.5).astype('int32')  \n",
    "# Get classification report\n",
    "# report = classification_report(y_test_lstm, y_pred.round(),target_names = ['0','1'])\n",
    "# print(\"Classification Report:\")\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM F1 score: 0.7350000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_test = f1_score(y_test_lstm, y_pred_binary )\n",
    "print(f'LSTM F1 score: {f1_test}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Combined text and numeric</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram and numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "numeric_features = X_train_full[['has_questions', 'required_experience', 'required_education', 'missing company information']]\n",
    "combined_features = hstack([\n",
    "    StandardScaler().fit_transform(numeric_features),\n",
    "    X_train_bicv])\n",
    "X_test_numeric_features = X_test_full[['has_questions', 'required_experience', 'required_education', 'missing company information']]\n",
    "X_test_combined_features = hstack([\n",
    "    StandardScaler().fit_transform(X_test_numeric_features),\n",
    "    X_test_bicv])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.8444444444444444\n",
      "SVM F1 Score: 0.7425474254742548\n",
      "Random Forest F1 Score: 0.8253164556962025\n",
      "XGBoost F1 Score: 0.8098765432098766\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_f1 = train_and_evaluate_model(logreg_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"Logistic Regression F1 Score: {logreg_f1}\")\n",
    "\n",
    "# Step 5: Support Vector Machine (SVM)\n",
    "svm_model = SVC()\n",
    "svm_f1 = train_and_evaluate_model(svm_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "\n",
    "# Step 6: Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_f1 = train_and_evaluate_model(rf_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_f1 = train_and_evaluate_model(xgb_model, combined_features, y_train, X_test_combined_features, y_test)\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Combined features</h3>\n",
    "\n",
    "|  | Logistic Regression | SVM | Random Forest | XGBoost |\n",
    "|----------|----------|----------|----------|----------|\n",
    "| Text Only | 0.832 | 0.739 | 0.817 | 0.796 |\n",
    "| Text and Numeric | 0.844 | 0.743 | 0.827 | 0.810 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
